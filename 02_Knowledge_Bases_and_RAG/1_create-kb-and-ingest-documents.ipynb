{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2 - Create Knowledge Base and Ingest Documents\n",
    "\n",
    "----\n",
    "\n",
    "This notebook provides sample code with step by step instructions for setting up an Amazon Bedrock Knowledge Base.\n",
    "\n",
    "----\n",
    "\n",
    "### Contents\n",
    "\n",
    "1. *Setup*\n",
    "1. *Create an S3 Data Source*\n",
    "1. *Setup AOSS Vector Index and Configure BKB Access Permissions*\n",
    "2. *Configure Amazon Bedrock Knowledge Base and Synchronize it with Data Source*\n",
    "3. *Conclusions and Next Steps*\n",
    "\n",
    "----\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Foundation models (FMs) are powerful AI models trained on vast amounts of general-purpose data. However, many real-world applications require these models to generate responses grounded in domain-specific or proprietary information. Retrieval Augmented Generation (RAG) is a technique that enhances generative AI responses by retrieving relevant information from external data sources at query time.\n",
    "\n",
    "Amazon Bedrock Knowledge Bases (BKBs) provide a fully managed capability to implement RAG-based solutions. By integrating your own data \u2014 such as documents, manuals, and other domain-specific sources of information \u2014 into a knowledge base, you can improve the accuracy, relevance, and usefulness of model-generated responses. When a user submits a query, Amazon Bedrock Knowledge Bases search across the available data sources, retrieve the most relevant content, and pass this information to the foundation model to generate a more informed response.\n",
    "\n",
    "![BKB illustration](./images/bkb_illustration.png)\n",
    "\n",
    "This notebook demonstrates how to create an empty Amazon OpenSearch Serverless (AOSS) index, build an Amazon Bedrock Knowledge Base, and ingest documents into it for retrieval-augmented generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Pre-requisites\n",
    "\n",
    "Please make sure that you have enabled the following model access in _Amazon Bedrock Console_:\n",
    "- `Amazon Titan Text Embeddings V2`.\n",
    "\n",
    "**If you are running AWS-facilitated event**, all other pre-requisites are satisfied and you can go to the next section.\n",
    "\n",
    "**If you are running this notebook as a self-paced lab**, then please note that this notebook requires permissions to:\n",
    "- create and delete *Amazon IAM* roles\n",
    "- create, update and delete *Amazon S3* buckets\n",
    "- access to *Amazon Bedrock*\n",
    "- access to *Amazon OpenSearch Serverless*\n",
    "\n",
    "In particular, if running on *SageMaker Studio*, you should add the following managed policies to your SageMaker execution role:\n",
    "- `IAMFullAccess`,\n",
    "- `AWSLambda_FullAccess`,\n",
    "- `AmazonS3FullAccess`,\n",
    "- `AmazonBedrockFullAccess`,\n",
    "- Custom policy for Amazon OpenSearch Serverless such as:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"aoss:*\",\n",
    "            \"Resource\": \"*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "````\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-flight Checklist\n",
    "\n",
    "Before starting this lab, let's verify your environment is properly configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive pre-flight checklist\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "checklist_status = {\n",
    "    \"prerequisites\": {\n",
    "        \"AWS Credentials\": False,\n",
    "        \"Bedrock Access\": False,\n",
    "        \"Titan Embeddings V2 Access\": False,\n",
    "        \"IAM Permissions\": False,\n",
    "        \"Region Verification\": False\n",
    "    },\n",
    "    \"warnings\": []\n",
    "}\n",
    "\n",
    "print(\"\ud83d\udd0d Running pre-flight checks...\\n\")\n",
    "\n",
    "# Check 1: AWS Credentials\n",
    "try:\n",
    "    boto3.client('sts').get_caller_identity()\n",
    "    checklist_status[\"prerequisites\"][\"AWS Credentials\"] = True\n",
    "    print(\"\u2705 AWS Credentials: Valid\")\n",
    "except Exception as e:\n",
    "    checklist_status[\"warnings\"].append(\"AWS credentials not configured\")\n",
    "    print(\"\u274c AWS Credentials: Invalid or not configured\")\n",
    "\n",
    "# Check 2: Region\n",
    "try:\n",
    "    region = boto3.Session().region_name\n",
    "    if region:\n",
    "        checklist_status[\"prerequisites\"][\"Region Verification\"] = True\n",
    "        print(f\"\u2705 Region: {region}\")\n",
    "    else:\n",
    "        checklist_status[\"warnings\"].append(\"AWS region not set\")\n",
    "        print(\"\u274c Region: Not configured\")\n",
    "except Exception as e:\n",
    "    print(\"\u274c Region: Cannot determine\")\n",
    "\n",
    "# Check 3: Bedrock Access\n",
    "try:\n",
    "    bedrock_client = boto3.client('bedrock')\n",
    "    bedrock_client.list_foundation_models(byProvider='amazon')\n",
    "    checklist_status[\"prerequisites\"][\"Bedrock Access\"] = True\n",
    "    print(\"\u2705 Bedrock Access: Available\")\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'AccessDeniedException':\n",
    "        checklist_status[\"warnings\"].append(\"No Bedrock access - check IAM permissions\")\n",
    "    print(\"\u274c Bedrock Access: Denied or unavailable\")\n",
    "except Exception as e:\n",
    "    print(\"\u274c Bedrock Access: Error checking access\")\n",
    "\n",
    "# Check 4: Model Access - Titan Embeddings V2\n",
    "try:\n",
    "    bedrock_client = boto3.client('bedrock')\n",
    "    response = bedrock_client.list_foundation_models(\n",
    "        byProvider='amazon',\n",
    "        byInferenceType='ON_DEMAND'\n",
    "    )\n",
    "    \n",
    "    titan_v2_available = False\n",
    "    for model in response.get('modelSummaries', []):\n",
    "        if 'amazon.titan-embed-text-v2' in model['modelId']:\n",
    "            titan_v2_available = True\n",
    "            break\n",
    "    \n",
    "    if titan_v2_available:\n",
    "        checklist_status[\"prerequisites\"][\"Titan Embeddings V2 Access\"] = True\n",
    "        print(\"\u2705 Titan Embeddings V2: Model access enabled\")\n",
    "    else:\n",
    "        checklist_status[\"warnings\"].append(\"Enable Titan Embeddings V2 in Bedrock Console\")\n",
    "        print(\"\u26a0\ufe0f  Titan Embeddings V2: Please enable model access in Bedrock Console\")\n",
    "except Exception as e:\n",
    "    print(\"\u274c Titan Embeddings V2: Cannot verify access\")\n",
    "\n",
    "# Check 5: IAM Permissions\n",
    "try:\n",
    "    iam_client = boto3.client('iam')\n",
    "    iam_client.list_roles(MaxItems=1)\n",
    "    checklist_status[\"prerequisites\"][\"IAM Permissions\"] = True\n",
    "    print(\"\u2705 IAM Permissions: Sufficient for role creation\")\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'AccessDenied':\n",
    "        checklist_status[\"warnings\"].append(\"Limited IAM permissions - may need IAMFullAccess\")\n",
    "        print(\"\u26a0\ufe0f  IAM Permissions: Limited (required for this lab)\")\n",
    "except Exception as e:\n",
    "    print(\"\u274c IAM Permissions: Cannot verify\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CHECKLIST SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "all_passed = all(checklist_status[\"prerequisites\"].values())\n",
    "\n",
    "if all_passed:\n",
    "    print(\"\ud83c\udf89 All checks passed! You're ready to proceed.\\n\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f  Some checks failed. Review the warnings below:\\n\")\n",
    "    for warning in checklist_status[\"warnings\"]:\n",
    "        print(f\"  - {warning}\")\n",
    "    print(\"\\nYou may still proceed, but expect potential errors.\\n\")\n",
    "\n",
    "print(\"Prerequisites status:\")\n",
    "for check, status in checklist_status[\"prerequisites\"].items():\n",
    "    status_icon = \"\u2705\" if status else \"\u274c\"\n",
    "    print(f\"  {status_icon} {check}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "### 1.1 Install and import the required libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --force-reinstall -q -r ./requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Third-party imports\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Local imports\n",
    "import utility\n",
    "\n",
    "# Print SDK versions\n",
    "print(f\"Python version: {sys.version.split()[0]}\")\n",
    "print(f\"Boto3 SDK version: {boto3.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Initial setup for clients and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boto3 session and set AWS region\n",
    "boto_session = boto3.Session()\n",
    "aws_region = boto_session.region_name\n",
    "\n",
    "# Create boto3 clients for AOSS, Bedrock, and S3 services\n",
    "aoss_client = boto3.client('opensearchserverless')\n",
    "bedrock_agent_client = boto3.client('bedrock-agent')\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Define names for AOSS, Bedrock, and S3 resources\n",
    "resource_suffix = random.randrange(100, 999)\n",
    "s3_bucket_name = f\"bedrock-kb-{aws_region}-{resource_suffix}\"\n",
    "aoss_collection_name = f\"bedrock-kb-collection-{resource_suffix}\"\n",
    "aoss_index_name = f\"bedrock-kb-index-{resource_suffix}\"\n",
    "bedrock_kb_name = f\"bedrock-kb-{resource_suffix}\"\n",
    "\n",
    "# Set the Bedrock model to use for embedding generation\n",
    "embedding_model_id = 'amazon.titan-embed-text-v2:0'\n",
    "embedding_model_arn = f'arn:aws:bedrock:{aws_region}::foundation-model/{embedding_model_id}'\n",
    "embedding_model_dim = 1024\n",
    "\n",
    "# Some temporary local paths\n",
    "local_data_dir = 'data'\n",
    "\n",
    "# Print configurations\n",
    "print(\"AWS Region:\", aws_region)\n",
    "print(\"S3 Bucket:\", s3_bucket_name)\n",
    "print(\"AOSS Collection Name:\", aoss_collection_name)\n",
    "print(\"Bedrock Knowledge Base Name:\", bedrock_kb_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create an S3 Data Source\n",
    "\n",
    "Amazon Bedrock Knowledge Bases can connect to a variety of data sources for downstream RAG applications. Supported data sources include Amazon S3, Confluence, Microsoft SharePoint, Salesforce, Web Crawler, and custom data sources.\n",
    "\n",
    "In this workshop, we will use Amazon S3 to store unstructured data \u2014 specifically, PDF files containing Amazon Shareholder Letters from different years. This S3 bucket will serve as the source of documents for our Knowledge Base. During the ingestion process, Bedrock will parse these documents, convert them into vector embeddings using an embedding model, and store them in a vector database for efficient retrieval during queries.\n",
    "\n",
    "### 2.1 Create an S3 bucket, if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if bucket exists, and if not create S3 bucket for KB data source\n",
    "max_attempts = 5\n",
    "attempt = 0\n",
    "\n",
    "while attempt < max_attempts:\n",
    "    try:\n",
    "        s3_client.head_bucket(Bucket=s3_bucket_name)\n",
    "        print(f\"Bucket '{s3_bucket_name}' already exists..\")\n",
    "        break\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == '404':\n",
    "            # Bucket doesn't exist in our account, try to create it\n",
    "            try:\n",
    "                print(f\"Creating bucket: '{s3_bucket_name}'..\")\n",
    "                if aws_region == 'us-east-1':\n",
    "                    s3_client.create_bucket(Bucket=s3_bucket_name)\n",
    "                else:\n",
    "                    s3_client.create_bucket(\n",
    "                        Bucket=s3_bucket_name,\n",
    "                        CreateBucketConfiguration={'LocationConstraint': aws_region}\n",
    "                    )\n",
    "                print(f\"Successfully created bucket: '{s3_bucket_name}'\")\n",
    "                break\n",
    "            except ClientError as create_error:\n",
    "                if create_error.response['Error']['Code'] == 'BucketAlreadyExists':\n",
    "                    # Bucket name is taken globally, generate a new one\n",
    "                    attempt += 1\n",
    "                    if attempt < max_attempts:\n",
    "                        # Generate a more unique bucket name\n",
    "                        resource_suffix = random.randrange(100000, 999999)\n",
    "                        timestamp_suffix = int(time.time())\n",
    "                        s3_bucket_name = f\"bedrock-kb-{aws_region}-{resource_suffix}-{timestamp_suffix}\"\n",
    "                        print(f\"Bucket name taken globally, trying new name: '{s3_bucket_name}'\")\n",
    "                        continue\n",
    "                    else:\n",
    "                        raise Exception(\"Failed to create a unique bucket name after 5 attempts. Please try running the notebook again.\")\n",
    "                else:\n",
    "                    # Re-raise other errors\n",
    "                    raise create_error\n",
    "        else:\n",
    "            # Re-raise other errors (like permission issues)\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Troubleshooting Common Issues\n",
    "\n",
    "As you progress through this workshop, you might encounter some common issues. Here's how to resolve them:\n",
    "\n",
    "#### Issue 1: BucketAlreadyExists Error\n",
    "\n",
    "**Symptom**:\n",
    "```\n",
    "ClientError: An error occurred (BucketAlreadyExists) when calling the CreateBucket operation: \n",
    "The requested bucket name is not available.\n",
    "```\n",
    "\n",
    "**Cause**: S3 bucket names are globally unique across ALL AWS accounts\n",
    "\n",
    "**Solution**: The notebook handles this automatically by:\n",
    "1. Catching the `BucketAlreadyExists` error\n",
    "2. Generating a new random suffix\n",
    "3. Retrying up to 5 times\n",
    "\n",
    "#### Issue 2: Access Denied (IAM Permissions)\n",
    "\n",
    "**Symptom**:\n",
    "```\n",
    "ClientError: An error occurred (AccessDeniedException) when calling the CreateRole operation: \n",
    "User is not authorized to perform: iam:CreateRole\n",
    "```\n",
    "\n",
    "**Cause**: Your IAM user/role lacks necessary permissions\n",
    "\n",
    "**Required Permissions**:\n",
    "- `iam:CreateRole`, `iam:AttachRolePolicy`, `iam:CreatePolicy`\n",
    "- `s3:CreateBucket`, `s3:PutObject`\n",
    "- `aoss:CreateCollection`, `aoss:CreateSecurityPolicy`\n",
    "- `bedrock:CreateKnowledgeBase`, `bedrock:InvokeModel`\n",
    "\n",
    "**Solutions**:\n",
    "1. **AWS Workshop**: Contact facilitator to grant permissions\n",
    "2. **Self-paced**: Add managed policies to your IAM role:\n",
    "   - `IAMFullAccess`\n",
    "   - `AmazonS3FullAccess`\n",
    "   - `AmazonBedrockFullAccess`\n",
    "   - Custom policy for AOSS (see prerequisites)\n",
    "\n",
    "#### Issue 3: Model Access Not Enabled\n",
    "\n",
    "**Symptom**:\n",
    "```\n",
    "ValidationException: The model 'amazon.titan-embed-text-v2:0' is not available.\n",
    "```\n",
    "\n",
    "**Solution**:\n",
    "1. Go to [Amazon Bedrock Console](https://console.aws.amazon.com/bedrock/)\n",
    "2. Click \"Model access\" in left navigation\n",
    "3. Enable \"Titan Text Embeddings V2\"\n",
    "4. Wait 2-3 minutes for access to propagate\n",
    "\n",
    "#### Issue 4: Ingestion Job Stuck or Failed\n",
    "\n",
    "**Possible Causes**:\n",
    "- S3 bucket permissions incorrect\n",
    "- PDF files corrupted\n",
    "- Bedrock service quota exceeded\n",
    "\n",
    "**Debugging**:\n",
    "```python\n",
    "response = bedrock_agent_client.get_ingestion_job(\n",
    "    knowledgeBaseId=bedrock_kb_id,\n",
    "    dataSourceId=bedrock_ds_id,\n",
    "    ingestionJobId=bedrock_job_id\n",
    ")\n",
    "print(\"Status:\", response['ingestionJob']['status'])\n",
    "print(\"Failures:\", response['ingestionJob'].get('failureReasons', []))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Understanding Workshop Costs\n",
    "\n",
    "Let's estimate the AWS costs for this workshop module to help you plan accordingly.\n",
    "\n",
    "#### Expected Costs (for completing all notebooks in Module 02)\n",
    "\n",
    "| Service | Usage | Estimated Cost | Notes |\n",
    "|---------|-------|---------------|-------|\n",
    "| **Amazon S3** | ~30 MB storage<br/>28 PDFs | ~$0.01 | $0.023/GB/month (first 50TB) |\n",
    "| **OpenSearch Serverless** | 2-3 hours active | ~$0.48 - $0.72 | $0.24/OCU-hour (4 OCU minimum) |\n",
    "| **Bedrock Embeddings** | ~600 chunks<br/>Titan V2 | ~$0.02 | $0.00002/token (input)<br/>~1M tokens total |\n",
    "| **Bedrock Inference** | ~20 queries<br/>Nova Micro | ~$0.01 | $0.035/1K input tokens<br/>$0.14/1K output tokens |\n",
    "| **Data Transfer** | Minimal | ~$0.00 | Within same region |\n",
    "| **IAM/CloudWatch** | Standard usage | $0.00 | No additional charges |\n",
    "| **TOTAL** | | **~$0.52 - $0.76** | **< $1 for complete module** |\n",
    "\n",
    "#### Cost Optimization Tips\n",
    "\n",
    "1. **Complete Workshop in One Session**: OpenSearch Serverless charges by the hour; finishing quickly reduces costs\n",
    "2. **Run Clean-up Notebook**: Delete resources when done (especially AOSS collection)\n",
    "3. **Use Same Region**: Avoid data transfer charges by keeping all resources in one region\n",
    "4. **Reuse Knowledge Base**: If experimenting, use the same KB instead of creating multiple\n",
    "\n",
    "#### Cost Breakdown by Notebook\n",
    "\n",
    "- **Notebook 1** (Setup & Ingestion): ~$0.50 (mostly AOSS creation + embeddings)\n",
    "- **Notebook 2** (Managed RAG): ~$0.01 (4 queries)\n",
    "- **Notebook 3** (Custom RAG): ~$0.01 (3 queries)\n",
    "- **Notebook 4** (Clean-up): $0.00 (resource deletion)\n",
    "\n",
    "**Important**: If you forget to run the clean-up notebook, the AOSS collection continues charging **$0.24/hour** (~$5.76/day) until deleted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Resource Naming Convention\n",
    "\n",
    "You may have noticed we're using a `resource_suffix` random number in our resource names. Let's understand why:\n",
    "\n",
    "#### Current Naming Pattern\n",
    "\n",
    "```python\n",
    "resource_suffix = random.randrange(100, 999)\n",
    "s3_bucket_name = f\"bedrock-kb-{aws_region}-{resource_suffix}\"\n",
    "aoss_collection_name = f\"bedrock-kb-collection-{resource_suffix}\"\n",
    "bedrock_kb_name = f\"bedrock-kb-{resource_suffix}\"\n",
    "```\n",
    "\n",
    "#### Why Random Suffixes?\n",
    "\n",
    "**Problem**: AWS resource names must be globally unique\n",
    "- S3 bucket names are unique across ALL AWS accounts worldwide\n",
    "- AOSS collection names must be unique within your account\n",
    "- If 50 students run this workshop simultaneously, conflicts will occur\n",
    "\n",
    "**Solution**: Random suffix prevents naming collisions\n",
    "- `resource_suffix = random.randrange(100, 999)` generates a 3-digit number\n",
    "- Each student gets a unique suffix (e.g., 347, 892, 156)\n",
    "- Probability of collision: ~0.2% with 50 students\n",
    "\n",
    "#### Resource Name Examples\n",
    "\n",
    "Your resources will be named like:\n",
    "```\n",
    "S3 Bucket: bedrock-kb-us-east-1-742\n",
    "AOSS Collection: bedrock-kb-collection-742\n",
    "Knowledge Base: bedrock-kb-742\n",
    "IAM Role: AmazonBedrockExecutionRoleForKnowledgeBase_742\n",
    "```\n",
    "\n",
    "#### Best Practices\n",
    "\n",
    "\u2705 **Use suffixes in workshops/shared environments**\n",
    "\u2705 **Include region in S3 bucket names** (aids troubleshooting)\n",
    "\u2705 **Use descriptive prefixes** (bedrock-kb- indicates purpose)\n",
    "\u274c **Don't use hardcoded names** in reusable notebooks\n",
    "\u274c **Don't use special characters** (stick to hyphens and underscores)\n",
    "\n",
    "#### Cleanup Consideration\n",
    "\n",
    "The random suffix ensures your resources don't conflict, but remember:\n",
    "- You must track your specific suffix to clean up later\n",
    "- We use Jupyter's `%store` magic to persist variables across notebooks\n",
    "- Always run notebook 4 (clean-up) to avoid orphaned resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Download data and upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "# List of shareholder-letter URLs (1997\u20132024)\n",
    "urls = [\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2025/ar/2024-Shareholder-Letter-Final.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2024/ar/Amazon-com-Inc-2023-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2023/ar/2022-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2022/ar/2021-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2021/ar/Amazon-2020-Shareholder-Letter-and-1997-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2020/ar/2019-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/annual/2018-Letter-to-Shareholders.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/annual/Amazon_Shareholder_Letter.pdf',  # 2017\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/annual/2016-Letter-to-Shareholders.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/annual/2015-Letter-to-Shareholders.PDF',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/annual/AMAZON-2014-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/annual/2013-Letter-to-Shareholders.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/annual/2012-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/annual/letter.PDF',  # 2011\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/annual/117006_ltr_ltr2.pdf',  # 2010\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/annual/AMZN_Shareholder-Letter-2009-(final).pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/annual/Amazon_SH_Letter_2008.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/annual/2007letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/annual/2006.PDF',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/annual/shareholderletter2005.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/annual/2004_shareholderLetter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/annual/2003_-Shareholder_-Letter041304.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/annual/2002_shareholderLetter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/annual/2001_shareholderLetter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/annual/00ar_letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/annual/Shareholderletter99.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/annual/Shareholderletter98.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/annual/Shareholderletter97.pdf',\n",
    "]\n",
    "\n",
    "# Corresponding output filenames\n",
    "filenames = [\n",
    "    'AMZN-2024-Shareholder-Letter.pdf',\n",
    "    'AMZN-2023-Shareholder-Letter.pdf',\n",
    "    'AMZN-2022-Shareholder-Letter.pdf',\n",
    "    'AMZN-2021-Shareholder-Letter.pdf',\n",
    "    'AMZN-2020-Shareholder-Letter-and-1997-Reprint.pdf',\n",
    "    'AMZN-2019-Shareholder-Letter.pdf',\n",
    "    'AMZN-2018-Shareholder-Letter.pdf',\n",
    "    'AMZN-2017-Shareholder-Letter.pdf',\n",
    "    'AMZN-2016-Shareholder-Letter.pdf',\n",
    "    'AMZN-2015-Shareholder-Letter.pdf',\n",
    "    'AMZN-2014-Shareholder-Letter.pdf',\n",
    "    'AMZN-2013-Shareholder-Letter.pdf',\n",
    "    'AMZN-2012-Shareholder-Letter.pdf',\n",
    "    'AMZN-2011-Shareholder-Letter.pdf',\n",
    "    'AMZN-2010-Shareholder-Letter.pdf',\n",
    "    'AMZN-2009-Shareholder-Letter.pdf',\n",
    "    'AMZN-2008-Shareholder-Letter.pdf',\n",
    "    'AMZN-2007-Shareholder-Letter.pdf',\n",
    "    'AMZN-2006-Shareholder-Letter.pdf',\n",
    "    'AMZN-2005-Shareholder-Letter.pdf',\n",
    "    'AMZN-2004-Shareholder-Letter.pdf',\n",
    "    'AMZN-2003-Shareholder-Letter.pdf',\n",
    "    'AMZN-2002-Shareholder-Letter.pdf',\n",
    "    'AMZN-2001-Shareholder-Letter.pdf',\n",
    "    'AMZN-2000-Shareholder-Letter.pdf',\n",
    "    'AMZN-1999-Shareholder-Letter.pdf',\n",
    "    'AMZN-1998-Shareholder-Letter.pdf',\n",
    "    'AMZN-1997-Shareholder-Letter.pdf',\n",
    "]\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(local_data_dir, exist_ok=True)\n",
    "\n",
    "# Download each file\n",
    "for url, filename in zip(urls, filenames):\n",
    "    file_path = os.path.join(local_data_dir, filename)\n",
    "    urlretrieve(url, file_path)\n",
    "    print(f\"Downloaded: '{filename}' to '{local_data_dir}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, _, files in os.walk(local_data_dir):\n",
    "    for file in files:\n",
    "        full_path = os.path.join(root, file)\n",
    "        s3_client.upload_file(full_path, s3_bucket_name, file)\n",
    "        print(f\"Uploaded: '{file}' to 's3://{s3_bucket_name}'..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 About the Amazon Shareholder Letters Dataset\n",
    "\n",
    "We've just downloaded 28 years of Amazon shareholder letters (1997-2024). Let's understand why this is an excellent dataset for RAG demonstrations.\n",
    "\n",
    "#### Dataset Characteristics\n",
    "\n",
    "**Volume & Timespan**:\n",
    "- 28 PDF documents spanning 28 years\n",
    "- ~150-200 pages total content\n",
    "- File size: ~30 MB total\n",
    "- Multiple CEO voices: Jeff Bezos (1997-2020), Andy Jassy (2021-2024)\n",
    "\n",
    "**Content Type**:\n",
    "- Annual business performance reviews\n",
    "- Strategic vision and long-term thinking\n",
    "- Financial metrics and growth statistics\n",
    "- Customer obsession principles\n",
    "- Technology innovation discussions (AWS, Alexa, Prime)\n",
    "\n",
    "#### Why This Dataset is Valuable for RAG\n",
    "\n",
    "1. **Temporal Dimension**: Tracks Amazon's evolution from bookstore to tech giant\n",
    "2. **Rich Context**: Business strategy, financial data, historical events\n",
    "3. **Real-World Complexity**: Mixed content (narratives, tables, financial data)\n",
    "4. **Educational Value**: Publicly available, well-written, relevant\n",
    "\n",
    "#### Example Queries This Dataset Supports\n",
    "\n",
    "**Factual**: \"What was Amazon's net income in 2023?\"\n",
    "**Conceptual**: \"What does Jeff Bezos mean by 'Day 1 thinking'?\"\n",
    "**Temporal**: \"How has R&D investment changed from 1997 to 2024?\"\n",
    "**Synthesis**: \"Summarize Amazon's AI strategy evolution\"\n",
    "\n",
    "#### Dataset Limitations\n",
    "\n",
    "\u26a0\ufe0f **Be Aware**:\n",
    "- Some letters include reprints (2020 includes 1997)\n",
    "- Financial data requires exact year specification\n",
    "- CEO transition in 2021 may affect response consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Setup AOSS Vector Index and Configure BKB Access Permissions\n",
    "\n",
    "In this section, we\u2019ll create a vector index using Amazon OpenSearch Serverless (AOSS) and configure the necessary access permissions for the Bedrock Knowledge Base (BKB) that we\u2019ll set up later. AOSS provides a fully managed, serverless solution for running vector search workloads at billion-vector scale. It automatically handles resource scaling and eliminates the need for cluster management, while delivering low-latency, millisecond response times with pay-per-use pricing.\n",
    "\n",
    "While this example uses AOSS, it\u2019s worth noting that Bedrock Knowledge Bases also supports other popular vector stores, including Amazon Aurora PostgreSQL with pgvector, Pinecone, Redis Enterprise Cloud, and MongoDB, among others\n",
    "\n",
    "### 3.1 Create IAM Role with Necessary Permissions for Bedrock Knowledge Base\n",
    "\n",
    "Let's first create an IAM role with all the necessary policies and permissions to allow BKB to execute operations, such as invoking Bedrock FMs and reading data from an S3 bucket. We will use a helper function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bedrock_kb_execution_role = utility.create_bedrock_execution_role(bucket_name=s3_bucket_name)\n",
    "bedrock_kb_execution_role_arn = bedrock_kb_execution_role['Role']['Arn']\n",
    "\n",
    "print(\"Created KB execution role with ARN:\", bedrock_kb_execution_role_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Create AOSS Policies and Vector Collection\n",
    "\n",
    "Next we need to create and attach three key policies for securing and managing access to the AOSS collection: an encryption policy, a network access policy, and a data access policy. These policies ensure proper encryption, network security, and the necessary permissions for creating, reading, updating, and deleting collection items and indexes. This step is essential for configuring the OpenSearch collection to interact with BKB securely and efficiently (you can read more about AOSS collections [here](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless.html)). We will use another helper function for this.\n",
    "\n",
    "\u26a0\ufe0f **Note:** _in order to keep setup overhead at mininum, in this example we **allow public internet access** to the OpenSearch Serverless collection resource. However, for production environments we strongly suggest to leverage private connection between your VPC and Amazon OpenSearch Serverless resources via an VPC endpoint, as described [here](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless-vpc.html)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create AOSS policies for the new vector collection\n",
    "aoss_encryption_policy, aoss_network_policy, aoss_access_policy = utility.create_policies_in_oss(\n",
    "    vector_store_name=aoss_collection_name,\n",
    "    aoss_client=aoss_client,\n",
    "    bedrock_kb_execution_role_arn=bedrock_kb_execution_role_arn)\n",
    "\n",
    "print(\"Created encryption policy with name:\", aoss_encryption_policy['securityPolicyDetail']['name'])\n",
    "print(\"Created network policy with name:\", aoss_network_policy['securityPolicyDetail']['name'])\n",
    "print(\"Created access policy with name:\", aoss_access_policy['accessPolicyDetail']['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the necessary policies in place, let's proceed to actually creating a new AOSS collection. Please note that this can take a **few minutes to complete**. While you wait, you may want to [explore the AOSS Console](https://console.aws.amazon.com/aos/home?#opensearch/collections), where you will see your AOSS collection being created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request to create AOSS collection\n",
    "aoss_collection = aoss_client.create_collection(name=aoss_collection_name, type='VECTORSEARCH')\n",
    "\n",
    "# Wait until collection becomes active with educational content\n",
    "print(\"Creating Amazon OpenSearch Serverless collection...\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"WHAT'S HAPPENING: AOSS Collection Provisioning\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "While we wait (2-3 minutes), here's what's happening behind the scenes:\n",
    "\n",
    "1. Resource Allocation:\n",
    "   - AWS is provisioning compute units (OCUs) for your collection\n",
    "   - Minimum 4 OCUs allocated (2 for indexing, 2 for search)\n",
    "   - Each OCU provides 6 GB memory + corresponding compute\n",
    "\n",
    "2. Security Configuration:\n",
    "   - Applying encryption policy (data encrypted at rest with AWS KMS)\n",
    "   - Enforcing network policy (public access enabled for workshop)\n",
    "   - Activating data access policy (permissions for Bedrock + your IAM user)\n",
    "\n",
    "3. Collection Initialization:\n",
    "   - Setting up OpenSearch cluster infrastructure\n",
    "   - Preparing vector search engine (FAISS-based HNSW algorithm)\n",
    "   - Creating API endpoints for data plane operations\n",
    "\n",
    "This serverless architecture means:\n",
    "\u2705 No cluster management needed\n",
    "\u2705 Automatic scaling based on workload\n",
    "\u2705 Pay only for resources used\n",
    "\u274c Initial provisioning latency (2-3 min)\n",
    "\"\"\")\n",
    "\n",
    "print(\"Progress: \", end='', flush=True)\n",
    "start_time = time.time()\n",
    "check_count = 0\n",
    "\n",
    "while True:\n",
    "    response = aoss_client.list_collections(collectionFilters={'name': aoss_collection_name})\n",
    "    status = response['collectionSummaries'][0]['status']\n",
    "    if status in ('ACTIVE', 'FAILED'):\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\" Done! ({elapsed:.1f} seconds)\")\n",
    "        break\n",
    "    print('\u2588', end='', flush=True)\n",
    "    check_count += 1\n",
    "    \n",
    "    # Educational milestones\n",
    "    if check_count == 12:  # ~60 seconds\n",
    "        print(\"\\n  \u23f1\ufe0f  1 minute elapsed - OCUs being allocated...\")\n",
    "        print(\"Progress: \", end='', flush=True)\n",
    "    elif check_count == 24:  # ~120 seconds\n",
    "        print(\"\\n  \u23f1\ufe0f  2 minutes elapsed - Security policies applying...\")\n",
    "        print(\"Progress: \", end='', flush=True)\n",
    "    \n",
    "    time.sleep(5)\n",
    "\n",
    "if status == 'ACTIVE':\n",
    "    print(\"\\n\u2705 AOSS collection is now ACTIVE and ready for indexing!\")\n",
    "    print(f\"   Collection ID: {response['collectionSummaries'][0]['id']}\")\n",
    "    print(f\"   Collection ARN: {response['collectionSummaries'][0]['arn']}\")\n",
    "else:\n",
    "    print(f\"\\n\u274c Collection creation FAILED with status: {status}\")\n",
    "\n",
    "print(\"An AOSS collection created:\", json.dumps(response['collectionSummaries'], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Grant BKB Access to AOSS Data\n",
    "\n",
    "In this step, we create a data access policy that grants BKB the necessary permissions to read from our AOSS collections. We then attach this policy to the Bedrock execution role we created earlier, allowing BKB to securely access AOSS data when generating responses. We will be using helper function once again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aoss_policy_arn = utility.create_oss_policy_attach_bedrock_execution_role(\n",
    "    collection_id=aoss_collection['createCollectionDetail']['id'],\n",
    "    bedrock_kb_execution_role=bedrock_kb_execution_role)\n",
    "\n",
    "print(\"Waiting 60 sec for data access rules to be enforced: \", end='')\n",
    "for _ in range(12):  # 12 * 5 sec = 60 sec\n",
    "    print('\u2588', end='', flush=True)\n",
    "    time.sleep(5)\n",
    "print(\" done.\")\n",
    "\n",
    "print(\"Created and attached policy with ARN:\", aoss_policy_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Create an AOSS Vector Index\n",
    "\n",
    "Now that we have all necessary access permissions in place, we can create a vector index in the AOSS collection we created previously.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests_aws4auth import AWS4Auth\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "\n",
    "# Use default credential configuration for authentication\n",
    "credentials = boto_session.get_credentials()\n",
    "awsauth = AWS4Auth(\n",
    "    credentials.access_key,\n",
    "    credentials.secret_key,\n",
    "    aws_region,\n",
    "    'aoss',\n",
    "    session_token=credentials.token)\n",
    "\n",
    "# Construct AOSS endpoint host\n",
    "host = f\"{aoss_collection['createCollectionDetail']['id']}.{aws_region}.aoss.amazonaws.com\"\n",
    "\n",
    "# Build the OpenSearch client\n",
    "os_client = OpenSearch(\n",
    "    hosts=[{'host': host, 'port': 443}],\n",
    "    http_auth=awsauth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection,\n",
    "    timeout=300\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to first define the index definiton with the desired indexing configuration, where we specify such things as number of shards and replicas of the index, vector embedding dimensions, the vector search engine (we are using FAISS here), as well as names and types of any other fields we need to have in the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the configuration for the AOSS vector index\n",
    "index_definition = {\n",
    "   \"settings\": {\n",
    "      \"index.knn\": \"true\",\n",
    "       \"number_of_shards\": 1,\n",
    "       \"knn.algo_param.ef_search\": 512,\n",
    "       \"number_of_replicas\": 0,\n",
    "   },\n",
    "   \"mappings\": {\n",
    "      \"properties\": {\n",
    "         \"vector\": {\n",
    "            \"type\": \"knn_vector\",\n",
    "            \"dimension\": embedding_model_dim,\n",
    "             \"method\": {\n",
    "                 \"name\": \"hnsw\",\n",
    "                 \"engine\": \"faiss\",\n",
    "                 \"space_type\": \"l2\"\n",
    "             },\n",
    "         },\n",
    "         \"text\": {\n",
    "            \"type\": \"text\"\n",
    "         },\n",
    "         \"text-metadata\": {\n",
    "            \"type\": \"text\"\n",
    "         }\n",
    "      }\n",
    "   }\n",
    "}\n",
    "\n",
    "# Create an OpenSearch index\n",
    "response = os_client.indices.create(index=aoss_index_name, body=index_definition)\n",
    "\n",
    "# Waiting for index creation to propagate\n",
    "print(\"Waiting 30 sec for index update to propagate: \", end='')\n",
    "for _ in range(6):  # 6 * 5 sec = 30 sec\n",
    "    print('\u2588', end='', flush=True)\n",
    "    time.sleep(5)\n",
    "print(\" done.\")\n",
    "\n",
    "print(\"A new AOSS index created:\", json.dumps(response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configure Amazon Bedrock Knowledge Base and Synchronize it with Data Source\n",
    "\n",
    "In this section, we\u2019ll create an Amazon Bedrock Knowledge Base (BKB) and connect it to the data that will be stored in our newly created AOSS vector index.\n",
    "\n",
    "### 4.1 Create a Bedrock Knowledge Base\n",
    "\n",
    "Setting up a Knowledge Base involves providing two key configurations:\n",
    "1. **Storage Configuration** tells Bedrock where to store the generated vector embeddings by specifying the target vector store and providing the necessary connection detail (here, we use the AOSS vector index we created earlier),\n",
    "2. **Knowledge Base Configuration** defines how Bedrock should generate vector embeddings from your data by specifying the embedding model to use (`Titan Text Embeddings V2` in this sample), along with any additional settings required for handling multimodal content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Knowledge Base Configuration\n",
    "knowledge_base_config = {\n",
    "    \"type\": \"VECTOR\",\n",
    "    \"vectorKnowledgeBaseConfiguration\": {\n",
    "        \"embeddingModelArn\": embedding_model_arn\n",
    "    }\n",
    "}\n",
    "\n",
    "# Storage Configuration  \n",
    "storage_config = {\n",
    "    \"type\": \"OPENSEARCH_SERVERLESS\",\n",
    "    \"opensearchServerlessConfiguration\": {\n",
    "        \"collectionArn\": aoss_collection['createCollectionDetail']['arn'],\n",
    "        \"vectorIndexName\": aoss_index_name,\n",
    "        \"fieldMapping\": {\n",
    "            \"vectorField\": \"vector\",\n",
    "            \"textField\": \"text\",\n",
    "            \"metadataField\": \"text-metadata\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Check if Knowledge Base already exists\n",
    "try:\n",
    "    # List existing knowledge bases to see if ours already exists\n",
    "    existing_kbs = bedrock_agent_client.list_knowledge_bases()\n",
    "    existing_kb = None\n",
    "    \n",
    "    for kb in existing_kbs['knowledgeBaseSummaries']:\n",
    "        if kb['name'] == bedrock_kb_name:\n",
    "            existing_kb = kb\n",
    "            break\n",
    "    \n",
    "    if existing_kb:\n",
    "        print(f\"Knowledge Base '{bedrock_kb_name}' already exists with ID: {existing_kb['knowledgeBaseId']}\")\n",
    "        print(\"Using existing Knowledge Base...\")\n",
    "        bedrock_kb_id = existing_kb['knowledgeBaseId']\n",
    "        \n",
    "        # Check if it's active\n",
    "        response = bedrock_agent_client.get_knowledge_base(knowledgeBaseId=bedrock_kb_id)\n",
    "        if response['knowledgeBase']['status'] == 'ACTIVE':\n",
    "            print(\"Existing Knowledge Base is already active.\")\n",
    "        else:\n",
    "            print(\"Waiting for existing Knowledge Base to become active: \", end='')\n",
    "            while True:\n",
    "                response = bedrock_agent_client.get_knowledge_base(knowledgeBaseId=bedrock_kb_id)\n",
    "                if response['knowledgeBase']['status'] == 'ACTIVE':\n",
    "                    print(\" done.\")\n",
    "                    break\n",
    "                print('\u2588', end='', flush=True)\n",
    "                time.sleep(5)\n",
    "    else:\n",
    "        # Create new Knowledge Base\n",
    "        response = bedrock_agent_client.create_knowledge_base(\n",
    "            name=bedrock_kb_name,\n",
    "            description=\"Amazon shareholder letter knowledge base.\",\n",
    "            roleArn=bedrock_kb_execution_role_arn,\n",
    "            knowledgeBaseConfiguration=knowledge_base_config,\n",
    "            storageConfiguration=storage_config)\n",
    "\n",
    "        bedrock_kb_id = response['knowledgeBase']['knowledgeBaseId']\n",
    "\n",
    "        print(\"Waiting until BKB becomes active: \", end='')\n",
    "        while True:\n",
    "            response = bedrock_agent_client.get_knowledge_base(knowledgeBaseId=bedrock_kb_id)\n",
    "            if response['knowledgeBase']['status'] == 'ACTIVE':\n",
    "                print(\" done.\")\n",
    "                break\n",
    "            print('\u2588', end='', flush=True)\n",
    "            time.sleep(5)\n",
    "\n",
    "        print(\"A new Bedrock Knowledge Base created with ID:\", bedrock_kb_id)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification: Confirm Knowledge Base configuration\n",
    "print(\"\ud83d\udd0d Verifying Knowledge Base Configuration...\\n\")\n",
    "\n",
    "try:\n",
    "    kb_details = bedrock_agent_client.get_knowledge_base(knowledgeBaseId=bedrock_kb_id)\n",
    "    kb = kb_details['knowledgeBase']\n",
    "    \n",
    "    # Check status\n",
    "    assert kb['status'] == 'ACTIVE', f\"Expected ACTIVE, got {kb['status']}\"\n",
    "    print(\"\u2705 Knowledge Base Status: ACTIVE\")\n",
    "    \n",
    "    # Check embedding model\n",
    "    expected_model = 'amazon.titan-embed-text-v2:0'\n",
    "    actual_model = kb['knowledgeBaseConfiguration']['vectorKnowledgeBaseConfiguration']['embeddingModelArn']\n",
    "    assert expected_model in actual_model, f\"Wrong embedding model: {actual_model}\"\n",
    "    print(f\"\u2705 Embedding Model: {expected_model}\")\n",
    "    \n",
    "    # Check vector store type\n",
    "    storage_type = kb['storageConfiguration']['type']\n",
    "    assert storage_type == 'OPENSEARCH_SERVERLESS', f\"Wrong storage type: {storage_type}\"\n",
    "    print(f\"\u2705 Vector Store: {storage_type}\")\n",
    "    \n",
    "    # Check AOSS configuration\n",
    "    aoss_config = kb['storageConfiguration']['opensearchServerlessConfiguration']\n",
    "    print(f\"\u2705 AOSS Collection ARN: {aoss_config['collectionArn']}\")\n",
    "    print(f\"\u2705 Vector Index Name: {aoss_config['vectorIndexName']}\")\n",
    "    \n",
    "    # Check IAM role\n",
    "    print(f\"\u2705 Execution Role ARN: {kb['roleArn']}\")\n",
    "    \n",
    "    print(\"\\n\ud83c\udf89 All verifications passed! Knowledge Base is properly configured.\")\n",
    "    \n",
    "except AssertionError as e:\n",
    "    print(f\"\u274c Verification failed: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"\u274c Error during verification: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call a Bedrock API to get the information about our newly created Knowledge Base:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock_agent_client.get_knowledge_base(knowledgeBaseId=bedrock_kb_id)\n",
    "\n",
    "print(json.dumps(response['knowledgeBase'], indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.5 Understanding Chunking Strategies\n",
    "\n",
    "Before we connect our Knowledge Base to a data source, it's crucial to understand **chunking strategies** \u2014 one of the most important configuration decisions in RAG systems.\n",
    "\n",
    "**What is Chunking?**\n",
    "\n",
    "When documents are ingested into a Knowledge Base, they cannot be stored as single, monolithic texts. Instead, they must be split into smaller \"chunks\" that can be:\n",
    "- Converted into vector embeddings\n",
    "- Retrieved independently based on semantic similarity\n",
    "- Injected into LLM prompts without exceeding context limits\n",
    "\n",
    "The chunking strategy determines **how** this splitting occurs, which directly impacts:\n",
    "- **Retrieval accuracy**: Whether the right information is found\n",
    "- **Response quality**: Whether the LLM has sufficient context\n",
    "- **Cost**: Number of chunks = number of embeddings stored\n",
    "- **Latency**: Smaller chunks = faster retrieval but potentially less context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Available Chunking Strategies in Amazon Bedrock Knowledge Bases\n",
    "\n",
    "Amazon Bedrock supports four chunking strategies:\n",
    "\n",
    "| Strategy | Description | Best For | Trade-offs |\n",
    "|----------|-------------|----------|------------|\n",
    "| **FIXED_SIZE** | Splits text into equal-sized chunks based on token count with configurable overlap | General-purpose documents, consistent structure | May split semantic units (sentences/paragraphs) |\n",
    "| **NONE** | No chunking; each document becomes one chunk | Short documents (<512 tokens), pre-chunked data | Not suitable for long documents; may exceed embedding limits |\n",
    "| **HIERARCHICAL** | Creates parent-child chunk relationships; retrieves child, returns parent context | Documents with clear structure (sections, chapters) | More complex, requires structured content |\n",
    "| **SEMANTIC** | Uses AI to identify natural semantic boundaries; chunks based on topic shifts | Unstructured content, narratives, varied topics | Higher ingestion cost, slower processing |\n",
    "\n",
    "**Current Configuration**: In this workshop, we use **FIXED_SIZE** with:\n",
    "- `maxTokens: 512` - Each chunk contains up to 512 tokens\n",
    "- `overlapPercentage: 20` - 20% overlap between adjacent chunks (prevents context loss at boundaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why Use Overlap?\n",
    "\n",
    "Consider this example from an Amazon shareholder letter:\n",
    "\n",
    "**Without Overlap (0%)**:\n",
    "- Chunk 1: \"...we launched Amazon Prime in 2005. This program transformed customer expectations\"\n",
    "- Chunk 2: \"and created unprecedented loyalty. Members shop more frequently...\"\n",
    "\n",
    "**With 20% Overlap**:\n",
    "- Chunk 1: \"...we launched Amazon Prime in 2005. This program transformed customer expectations and created unprecedented loyalty.\"\n",
    "- Chunk 2: \"This program transformed customer expectations and created unprecedented loyalty. Members shop more frequently...\"\n",
    "\n",
    "**Query**: \"How did Amazon Prime affect customer loyalty?\"\n",
    "\n",
    "With overlap, Chunk 2 includes the critical context that \"this program\" refers to Amazon Prime, improving retrieval accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practical Comparison: Shareholder Letters\n",
    "\n",
    "Let's analyze how different strategies would handle our Amazon shareholder letters:\n",
    "\n",
    "**Example Text** (from 2024 letter):\n",
    "> \"Our AWS business continued its strong momentum in Q4, with revenue reaching $26.3 billion, up 13% year-over-year. The AI and machine learning services within AWS saw particularly strong adoption, with customers leveraging Amazon Bedrock and SageMaker for their generative AI initiatives.\"\n",
    "\n",
    "**FIXED_SIZE (512 tokens, 20% overlap)**:\n",
    "- \u2705 Predictable chunk sizes\n",
    "- \u2705 Works well for financial data with numbers\n",
    "- \u26a0\ufe0f Might split \"AWS revenue\" from \"year-over-year comparison\"\n",
    "\n",
    "**SEMANTIC**:\n",
    "- \u2705 Keeps \"AWS revenue\" and \"year-over-year growth\" together\n",
    "- \u2705 Groups related services (Bedrock, SageMaker) in one chunk\n",
    "- \u26a0\ufe0f Variable chunk sizes might affect consistency\n",
    "\n",
    "**HIERARCHICAL**:\n",
    "- \u2705 Could treat each year's letter as parent, each section as child\n",
    "- \u2705 Retrieves specific metric but returns full section context\n",
    "- \u26a0\ufe0f Requires structured PDFs with clear section markers\n",
    "\n",
    "**NONE**:\n",
    "- \u274c Each full shareholder letter (5-10 pages) becomes one chunk\n",
    "- \u274c Exceeds typical embedding model limits (512-1024 tokens)\n",
    "- \u274c Not recommended for this use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEMONSTRATION: How different chunking strategies affect the same text\n",
    "# Note: This is a simulation for educational purposes\n",
    "\n",
    "sample_text = \"\"\"\n",
    "Amazon Web Services (AWS) continued to show strong performance throughout 2024. \n",
    "The cloud infrastructure business grew revenue by 13% year-over-year, reaching $26.3 billion in Q4 alone.\n",
    "\n",
    "Our AI and machine learning services saw exceptional adoption. Amazon Bedrock, our fully managed \n",
    "generative AI service, enabled thousands of customers to build innovative applications. SageMaker \n",
    "customers increased by 45% as organizations accelerated their ML initiatives.\n",
    "\n",
    "Looking forward, we remain focused on three key areas: expanding our infrastructure footprint, \n",
    "enhancing our AI/ML capabilities, and deepening customer relationships through innovation.\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "\n",
    "def simulate_fixed_size_chunking(text, max_tokens=50, overlap_pct=20):\n",
    "    \"\"\"Simulates FIXED_SIZE chunking strategy\"\"\"\n",
    "    # Simplified: using words as proxy for tokens (1 word \u2248 1.3 tokens)\n",
    "    words = text.split()\n",
    "    max_words = int(max_tokens / 1.3)\n",
    "    overlap_words = int(max_words * overlap_pct / 100)\n",
    "    \n",
    "    chunks = []\n",
    "    i = 0\n",
    "    chunk_num = 1\n",
    "    \n",
    "    while i < len(words):\n",
    "        chunk_end = min(i + max_words, len(words))\n",
    "        chunk = ' '.join(words[i:chunk_end])\n",
    "        chunks.append(f\"Chunk {chunk_num}: {chunk}...\")\n",
    "        \n",
    "        # Move forward by (chunk_size - overlap)\n",
    "        i += (max_words - overlap_words)\n",
    "        chunk_num += 1\n",
    "        \n",
    "        if i >= len(words):\n",
    "            break\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def simulate_semantic_chunking(text):\n",
    "    \"\"\"Simulates SEMANTIC chunking strategy\"\"\"\n",
    "    # Simplified: splits on paragraph boundaries (semantic breaks)\n",
    "    paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
    "    chunks = [f\"Semantic Chunk {i+1}: {p}\" for i, p in enumerate(paragraphs)]\n",
    "    return chunks\n",
    "\n",
    "def simulate_no_chunking(text):\n",
    "    \"\"\"Simulates NONE strategy\"\"\"\n",
    "    return [f\"Single Chunk (NONE strategy): {text}\"]\n",
    "\n",
    "# Compare strategies\n",
    "print(\"=\"*80)\n",
    "print(\"FIXED_SIZE Chunking (50 tokens, 20% overlap)\")\n",
    "print(\"=\"*80)\n",
    "for chunk in simulate_fixed_size_chunking(sample_text):\n",
    "    print(f\"\\n{chunk}\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SEMANTIC Chunking (natural boundaries)\")\n",
    "print(\"=\"*80)\n",
    "for chunk in simulate_semantic_chunking(sample_text):\n",
    "    print(f\"\\n{chunk}\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"NONE Strategy (no chunking)\")\n",
    "print(\"=\"*80)\n",
    "for chunk in simulate_no_chunking(sample_text):\n",
    "    print(f\"\\n{chunk}\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Analysis:\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "FIXED_SIZE:\n",
    "  - Created multiple overlapping chunks\n",
    "  - Consistent size enables predictable retrieval\n",
    "  - Overlap preserves context at boundaries\n",
    "\n",
    "SEMANTIC:\n",
    "  - Aligned with natural topic breaks\n",
    "  - Variable sizes (AWS performance, AI/ML services, Future focus)\n",
    "  - Each chunk is semantically complete\n",
    "\n",
    "NONE:\n",
    "  - Entire text as single chunk\n",
    "  - Only suitable if text is already small (<512 tokens)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chunking Best Practices\n",
    "\n",
    "**For Financial Documents (like shareholder letters)**:\n",
    "- \u2705 **Recommended**: FIXED_SIZE with 512 tokens, 20% overlap\n",
    "- **Rationale**: Financial data benefits from consistent chunk sizes; overlap ensures metrics stay with their context\n",
    "- **Alternative**: SEMANTIC for narrative sections, but FIXED_SIZE for tables/metrics\n",
    "\n",
    "**General Guidelines**:\n",
    "\n",
    "1. **Chunk Size Selection**:\n",
    "   - **300-512 tokens**: Optimal for most use cases (balances context vs. precision)\n",
    "   - **Less than 300**: Risk of insufficient context\n",
    "   - **More than 800**: Risk of retrieving irrelevant information\n",
    "\n",
    "2. **Overlap Percentage**:\n",
    "   - **15-20%**: Standard overlap for general content\n",
    "   - **30-40%**: Higher overlap for dense technical content\n",
    "   - **0-10%**: Lower overlap for clearly structured documents\n",
    "\n",
    "3. **Strategy Selection Decision Tree**:\n",
    "   ```\n",
    "   Is your content pre-chunked? (e.g., Q&A pairs)\n",
    "       YES \u2192 Use NONE\n",
    "       NO \u2192 Continue\n",
    "   \n",
    "   Does your content have clear hierarchical structure? (e.g., documentation with sections)\n",
    "       YES \u2192 Consider HIERARCHICAL\n",
    "       NO \u2192 Continue\n",
    "   \n",
    "   Is your content highly unstructured with varying topics? (e.g., emails, transcripts)\n",
    "       YES \u2192 Consider SEMANTIC\n",
    "       NO \u2192 Use FIXED_SIZE (safest default)\n",
    "   ```\n",
    "\n",
    "4. **Cost Considerations**:\n",
    "   - Smaller chunks = More embeddings = Higher storage cost\n",
    "   - Balance chunk size with retrieval quality needs\n",
    "   - For our 28 shareholder letters: ~500-700 chunks expected with FIXED_SIZE\n",
    "\n",
    "#### What Happens Next?\n",
    "\n",
    "When we create our data source in the next cell, Bedrock will:\n",
    "1. Read each PDF from S3\n",
    "2. Extract text content\n",
    "3. Apply the FIXED_SIZE chunking strategy (512 tokens, 20% overlap)\n",
    "4. Generate embeddings for each chunk using Titan Embeddings V2\n",
    "5. Store embeddings in our AOSS vector index\n",
    "\n",
    "Let's proceed to configure the data source!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \ud83c\udfaf Learning Checkpoint: Chunking Strategies\n",
    "\n",
    "Before proceeding, test your understanding:\n",
    "\n",
    "**Question 1**: If a document is 2,000 tokens long and we use FIXED_SIZE chunking with maxTokens=512 and overlapPercentage=20, approximately how many chunks will be created?\n",
    "\n",
    "<details>\n",
    "<summary>Click to see answer</summary>\n",
    "\n",
    "**Answer**: Approximately 4-5 chunks\n",
    "\n",
    "**Explanation**:\n",
    "- Each chunk is 512 tokens\n",
    "- Overlap is 20% = ~102 tokens\n",
    "- Effective advancement per chunk = 512 - 102 = 410 tokens\n",
    "- 2000 / 410 \u2248 4.9 chunks\n",
    "\n",
    "Calculation:\n",
    "- Chunk 1: tokens 0-512\n",
    "- Chunk 2: tokens 410-922 (102 overlap from chunk 1)\n",
    "- Chunk 3: tokens 820-1332\n",
    "- Chunk 4: tokens 1230-1742\n",
    "- Chunk 5: tokens 1640-2000 (partial)\n",
    "\n",
    "</details>\n",
    "\n",
    "**Question 2**: Why might SEMANTIC chunking be slower than FIXED_SIZE during ingestion?\n",
    "\n",
    "<details>\n",
    "<summary>Click to see answer</summary>\n",
    "\n",
    "**Answer**: SEMANTIC chunking uses AI models to identify topic boundaries, requiring additional inference calls.\n",
    "\n",
    "**Explanation**:\n",
    "- FIXED_SIZE: Simple token counting (fast)\n",
    "- SEMANTIC: Analyzes text semantics using ML models (slower but more intelligent)\n",
    "- Trade-off: Better semantic coherence vs. longer ingestion time\n",
    "\n",
    "</details>\n",
    "\n",
    "**Question 3**: When would you choose NONE as your chunking strategy?\n",
    "\n",
    "<details>\n",
    "<summary>Click to see answer</summary>\n",
    "\n",
    "**Answer**: When documents are already small (<512 tokens) or pre-chunked into semantic units.\n",
    "\n",
    "**Examples**:\n",
    "- FAQ pairs where each Q&A is already a complete unit\n",
    "- Product descriptions (50-200 tokens each)\n",
    "- Pre-processed data where you've already done custom chunking\n",
    "- Email subjects and bodies stored separately\n",
    "\n",
    "**Warning**: NONE with large documents will fail if they exceed embedding model limits.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Connect BKB to a Data Source\n",
    "\n",
    "With our Knowledge Base in place, the next step is to connect it to a data source. This involves two key actions:\n",
    "\n",
    "1. **Create a data source for the Knowledge Base** that will point to the location of our raw data (in this case, S3),\n",
    "2. **Define how that data should be processed and ingested into the vector store** \u2014 for example, by specifying a chunking configuration that controls how large each text fragment should be when generating vector embeddings for retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Source Configuration\n",
    "data_source_config = {\n",
    "        \"type\": \"S3\",\n",
    "        \"s3Configuration\":{\n",
    "            \"bucketArn\": f\"arn:aws:s3:::{s3_bucket_name}\",\n",
    "            # \"inclusionPrefixes\":[\"*.*\"]   # you can use this if you want to create a KB using data within s3 prefixes.\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Vector Ingestion Configuration\n",
    "vector_ingestion_config = {\n",
    "        \"chunkingConfiguration\": {\n",
    "            \"chunkingStrategy\": \"FIXED_SIZE\",\n",
    "            \"fixedSizeChunkingConfiguration\": {\n",
    "                \"maxTokens\": 512,\n",
    "                \"overlapPercentage\": 20\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "response = bedrock_agent_client.create_data_source(\n",
    "    name=bedrock_kb_name,\n",
    "    description=\"Amazon shareholder letter knowledge base.\",\n",
    "    knowledgeBaseId=bedrock_kb_id,\n",
    "    dataSourceConfiguration=data_source_config,\n",
    "    vectorIngestionConfiguration=vector_ingestion_config\n",
    ")\n",
    "\n",
    "bedrock_ds_id = response['dataSource']['dataSourceId']\n",
    "\n",
    "print(\"A new BKB data source created with ID:\", bedrock_ds_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also use Bedrock API to get the information about our newly created BKB data source:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = bedrock_agent_client.get_data_source(knowledgeBaseId=bedrock_kb_id, dataSourceId=bedrock_ds_id)\n",
    "\n",
    "print(json.dumps(response['dataSource'], indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Synchronize BKB with Data Source\n",
    "\n",
    "Once the Knowledge Base and its data source are configured, we can start a fully-managed data ingestion job. During this process, BKB will retrieve the documents from the connected data source (on S3, in this case), extract and preprocess the content, split it into smaller chunks based on the configured chunking strategy, generate vector embeddings for each chunk, and store those embeddings in the vector store (AOSS vector store, in this case).\n",
    "\n",
    "![BKB data ingestion](./images/data_ingestion.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start an ingestion job\n",
    "response = bedrock_agent_client.start_ingestion_job(knowledgeBaseId=bedrock_kb_id, dataSourceId=bedrock_ds_id)\n",
    "\n",
    "bedrock_job_id = response['ingestionJob']['ingestionJobId']\n",
    "\n",
    "print(\"A new BKB ingestion job started with ID:\", bedrock_job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Wait until ingestion job completes\n",
    "print(\"Starting Knowledge Base ingestion job...\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"WHAT'S HAPPENING: Document Ingestion Pipeline\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "Processing 28 Amazon shareholder letters (1997-2024). This typically takes 5-7 minutes.\n",
    "\n",
    "The Ingestion Pipeline:\n",
    "\n",
    " S3 \u2192 Parse \u2192 Extract \u2192 Chunk \u2192 Embed \u2192 Index \u2192 Validate\n",
    "\n",
    "Step-by-Step Process:\n",
    "\n",
    "1. Document Retrieval (30 sec):\n",
    "   - Bedrock downloads all 28 PDFs from S3\n",
    "   - Validates file integrity and permissions\n",
    "\n",
    "2. Content Extraction (1-2 min):\n",
    "   - PDF parsing (text extraction, layout analysis)\n",
    "   - Removes headers, footers, page numbers\n",
    "   - Handles multi-column layouts and tables\n",
    "\n",
    "3. Chunking Strategy Application (30 sec):\n",
    "   - Splits text using FIXED_SIZE strategy\n",
    "   - Creates chunks of 512 tokens with 20% overlap\n",
    "   - Expected output: ~600-700 chunks from 28 letters\n",
    "\n",
    "4. Embedding Generation (3-4 min) - SLOWEST STEP:\n",
    "   - Each chunk sent to Titan Embeddings V2 model\n",
    "   - Generates 1,024-dimensional vectors\n",
    "   - Batch processing: ~10-20 chunks/second\n",
    "\n",
    "5. Vector Indexing (1 min):\n",
    "   - Inserts vectors into AOSS collection\n",
    "   - Builds HNSW graph for similarity search\n",
    "\n",
    "6. Validation & Finalization (10 sec):\n",
    "   - Verifies all documents processed successfully\n",
    "   - Updates Knowledge Base status\n",
    "\n",
    "Cost: ~$0.02 for 600 chunks \u00d7 1024 dimensions\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nProgress: \", end='', flush=True)\n",
    "start_time = time.time()\n",
    "check_count = 0\n",
    "\n",
    "while True:\n",
    "    response = bedrock_agent_client.get_ingestion_job(\n",
    "        knowledgeBaseId = bedrock_kb_id,\n",
    "        dataSourceId = bedrock_ds_id,\n",
    "        ingestionJobId = bedrock_job_id)\n",
    "    \n",
    "    status = response['ingestionJob']['status']\n",
    "    \n",
    "    if status == 'COMPLETE':\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\" Done! ({elapsed:.1f} seconds)\")\n",
    "        break\n",
    "    elif status == 'FAILED':\n",
    "        print(f\"\\n\u274c Ingestion FAILED\")\n",
    "        break\n",
    "    \n",
    "    print('\u2588', end='', flush=True)\n",
    "    check_count += 1\n",
    "    \n",
    "    # Educational milestones\n",
    "    if check_count == 12:  # ~60 seconds\n",
    "        print(\"\\n  \u23f1\ufe0f  1 minute - Document retrieval complete, extracting text...\")\n",
    "        print(\"Progress: \", end='', flush=True)\n",
    "    elif check_count == 36:  # ~180 seconds\n",
    "        print(\"\\n  \u23f1\ufe0f  3 minutes - Chunking complete, generating embeddings (~60% done)...\")\n",
    "        print(\"Progress: \", end='', flush=True)\n",
    "    elif check_count == 60:  # ~300 seconds\n",
    "        print(\"\\n  \u23f1\ufe0f  5 minutes - Embedding generation complete, indexing vectors...\")\n",
    "        print(\"Progress: \", end='', flush=True)\n",
    "    \n",
    "    time.sleep(5)\n",
    "\n",
    "# Display statistics\n",
    "if status == 'COMPLETE':\n",
    "    stats = response['ingestionJob'].get('statistics', {})\n",
    "    print(\"\\n\u2705 Ingestion Complete!\")\n",
    "    print(f\"\\n\ud83d\udcca Ingestion Statistics:\")\n",
    "    print(f\"   Documents Processed: {stats.get('numberOfDocumentsScanned', 'N/A')}\")\n",
    "    print(f\"   Documents Failed: {stats.get('numberOfDocumentsFailed', 0)}\")\n",
    "    if 'numberOfChunks' in stats:\n",
    "        print(f\"   Chunks Created: {stats['numberOfChunks']}\")\n",
    "    print(f\"   Total Time: {elapsed:.1f} seconds ({elapsed/60:.1f} minutes)\")\n",
    "\n",
    "print(\"\\nFull job details:\", json.dumps(response['ingestionJob'], indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusions and Next Steps\n",
    "\n",
    "In this notebook, we walked through the process of creating an Amazon Bedrock Knowledge Base (BKB) and ingesting documents to enable Retrieval Augmented Generation (RAG) capabilities. We started by setting up the environment, installing the required libraries, and initializing the necessary AWS service clients. Then, we created an Amazon S3 bucket to store unstructured data (PDF documents) and uploaded sample files. We proceeded by provisioning an Amazon OpenSearch Serverless (AOSS) collection and index, configuring the appropriate IAM roles and permissions, and granting access to the BKB. Finally, we created the BKB, connected it to the S3 data source, and synchronized the documents to generate vector embeddings, which were stored in AOSS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "Please execute next cell to store some important varables that will be needed in other notebooks of this module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store s3_bucket_name aoss_encryption_policy aoss_network_policy aoss_access_policy aoss_collection bedrock_kb_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now please go on to explore how you can interact with the newly created Knowledge Base via Bedrock APIs for RAG applications, please proceed to the next notebook:\n",
    "\n",
    "&nbsp; **NEXT \u25b6** [2_managed-rag-with-retrieve-and-generate-api.ipynb](./2\\_managed-rag-with-retrieve-and-generate-api.ipynb)."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}