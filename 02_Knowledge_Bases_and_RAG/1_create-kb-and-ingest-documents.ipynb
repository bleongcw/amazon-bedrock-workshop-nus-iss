{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2 - Create Knowledge Base and Ingest Documents\n",
    "\n",
    "----\n",
    "\n",
    "This notebook provides sample code with step by step instructions for setting up an Amazon Bedrock Knowledge Base.\n",
    "\n",
    "----\n",
    "\n",
    "### Contents\n",
    "\n",
    "1. *Setup*\n",
    "1. *Create an S3 Data Source*\n",
    "1. *Setup AOSS Vector Index and Configure BKB Access Permissions*\n",
    "2. *Configure Amazon Bedrock Knowledge Base and Synchronize it with Data Source*\n",
    "3. *Conclusions and Next Steps*\n",
    "\n",
    "----\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Foundation models (FMs) are powerful AI models trained on vast amounts of general-purpose data. However, many real-world applications require these models to generate responses grounded in domain-specific or proprietary information. Retrieval Augmented Generation (RAG) is a technique that enhances generative AI responses by retrieving relevant information from external data sources at query time.\n",
    "\n",
    "Amazon Bedrock Knowledge Bases (BKBs) provide a fully managed capability to implement RAG-based solutions. By integrating your own data \u2014 such as documents, manuals, and other domain-specific sources of information \u2014 into a knowledge base, you can improve the accuracy, relevance, and usefulness of model-generated responses. When a user submits a query, Amazon Bedrock Knowledge Bases search across the available data sources, retrieve the most relevant content, and pass this information to the foundation model to generate a more informed response.\n",
    "\n",
    "![BKB illustration](./images/bkb_illustration.png)\n",
    "\n",
    "This notebook demonstrates how to create an empty Amazon OpenSearch Serverless (AOSS) index, build an Amazon Bedrock Knowledge Base, and ingest documents into it for retrieval-augmented generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Pre-requisites\n",
    "\n",
    "Please make sure that you have enabled the following model access in _Amazon Bedrock Console_:\n",
    "- `Amazon Titan Text Embeddings V2`.\n",
    "\n",
    "**If you are running AWS-facilitated event**, all other pre-requisites are satisfied and you can go to the next section.\n",
    "\n",
    "**If you are running this notebook as a self-paced lab**, then please note that this notebook requires permissions to:\n",
    "- create and delete *Amazon IAM* roles\n",
    "- create, update and delete *Amazon S3* buckets\n",
    "- access to *Amazon Bedrock*\n",
    "- access to *Amazon OpenSearch Serverless*\n",
    "\n",
    "In particular, if running on *SageMaker Studio*, you should add the following managed policies to your SageMaker execution role:\n",
    "- `IAMFullAccess`,\n",
    "- `AWSLambda_FullAccess`,\n",
    "- `AmazonS3FullAccess`,\n",
    "- `AmazonBedrockFullAccess`,\n",
    "- Custom policy for Amazon OpenSearch Serverless such as:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"aoss:*\",\n",
    "            \"Resource\": \"*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "````\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "### 1.1 Install and import the required libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --force-reinstall -q -r ./requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Third-party imports\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Local imports\n",
    "import utility\n",
    "\n",
    "# Print SDK versions\n",
    "print(f\"Python version: {sys.version.split()[0]}\")\n",
    "print(f\"Boto3 SDK version: {boto3.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Initial setup for clients and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boto3 session and set AWS region\n",
    "boto_session = boto3.Session()\n",
    "aws_region = boto_session.region_name\n",
    "\n",
    "# Create boto3 clients for AOSS, Bedrock, and S3 services\n",
    "aoss_client = boto3.client('opensearchserverless')\n",
    "bedrock_agent_client = boto3.client('bedrock-agent')\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Define names for AOSS, Bedrock, and S3 resources\n",
    "resource_suffix = random.randrange(100, 999)\n",
    "s3_bucket_name = f\"bedrock-kb-{aws_region}-{resource_suffix}\"\n",
    "aoss_collection_name = f\"bedrock-kb-collection-{resource_suffix}\"\n",
    "aoss_index_name = f\"bedrock-kb-index-{resource_suffix}\"\n",
    "bedrock_kb_name = f\"bedrock-kb-{resource_suffix}\"\n",
    "\n",
    "# Set the Bedrock model to use for embedding generation\n",
    "embedding_model_id = 'amazon.titan-embed-text-v2:0'\n",
    "embedding_model_arn = f'arn:aws:bedrock:{aws_region}::foundation-model/{embedding_model_id}'\n",
    "embedding_model_dim = 1024\n",
    "\n",
    "# Some temporary local paths\n",
    "local_data_dir = 'data'\n",
    "\n",
    "# Print configurations\n",
    "print(\"AWS Region:\", aws_region)\n",
    "print(\"S3 Bucket:\", s3_bucket_name)\n",
    "print(\"AOSS Collection Name:\", aoss_collection_name)\n",
    "print(\"Bedrock Knowledge Base Name:\", bedrock_kb_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create an S3 Data Source\n",
    "\n",
    "Amazon Bedrock Knowledge Bases can connect to a variety of data sources for downstream RAG applications. Supported data sources include Amazon S3, Confluence, Microsoft SharePoint, Salesforce, Web Crawler, and custom data sources.\n",
    "\n",
    "In this workshop, we will use Amazon S3 to store unstructured data \u2014 specifically, PDF files containing Amazon Shareholder Letters from different years. This S3 bucket will serve as the source of documents for our Knowledge Base. During the ingestion process, Bedrock will parse these documents, convert them into vector embeddings using an embedding model, and store them in a vector database for efficient retrieval during queries.\n",
    "\n",
    "### 2.1 Create an S3 bucket, if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if bucket exists, and if not create S3 bucket for KB data source\n",
    "max_attempts = 5\n",
    "attempt = 0\n",
    "\n",
    "while attempt < max_attempts:\n",
    "    try:\n",
    "        s3_client.head_bucket(Bucket=s3_bucket_name)\n",
    "        print(f\"Bucket '{s3_bucket_name}' already exists..\")\n",
    "        break\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == '404':\n",
    "            # Bucket doesn't exist in our account, try to create it\n",
    "            try:\n",
    "                print(f\"Creating bucket: '{s3_bucket_name}'..\")\n",
    "                if aws_region == 'us-east-1':\n",
    "                    s3_client.create_bucket(Bucket=s3_bucket_name)\n",
    "                else:\n",
    "                    s3_client.create_bucket(\n",
    "                        Bucket=s3_bucket_name,\n",
    "                        CreateBucketConfiguration={'LocationConstraint': aws_region}\n",
    "                    )\n",
    "                print(f\"Successfully created bucket: '{s3_bucket_name}'\")\n",
    "                break\n",
    "            except ClientError as create_error:\n",
    "                if create_error.response['Error']['Code'] == 'BucketAlreadyExists':\n",
    "                    # Bucket name is taken globally, generate a new one\n",
    "                    attempt += 1\n",
    "                    if attempt < max_attempts:\n",
    "                        # Generate a more unique bucket name\n",
    "                        resource_suffix = random.randrange(100000, 999999)\n",
    "                        timestamp_suffix = int(time.time())\n",
    "                        s3_bucket_name = f\"bedrock-kb-{aws_region}-{resource_suffix}-{timestamp_suffix}\"\n",
    "                        print(f\"Bucket name taken globally, trying new name: '{s3_bucket_name}'\")\n",
    "                        continue\n",
    "                    else:\n",
    "                        raise Exception(\"Failed to create a unique bucket name after 5 attempts. Please try running the notebook again.\")\n",
    "                else:\n",
    "                    # Re-raise other errors\n",
    "                    raise create_error\n",
    "        else:\n",
    "            # Re-raise other errors (like permission issues)\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Download data and upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "# List of shareholder-letter URLs (1997\u20132024)\n",
    "urls = [\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2025/ar/2024-Shareholder-Letter-Final.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2024/ar/Amazon-com-Inc-2023-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2023/ar/2022-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2022/ar/2021-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2021/ar/Amazon-2020-Shareholder-Letter-and-1997-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2020/ar/2019-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/annual/2018-Letter-to-Shareholders.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/annual/Amazon_Shareholder_Letter.pdf',  # 2017\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/annual/2016-Letter-to-Shareholders.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/annual/2015-Letter-to-Shareholders.PDF',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/annual/AMAZON-2014-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/annual/2013-Letter-to-Shareholders.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/annual/2012-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/annual/letter.PDF',  # 2011\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/annual/117006_ltr_ltr2.pdf',  # 2010\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/annual/AMZN_Shareholder-Letter-2009-(final).pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/annual/Amazon_SH_Letter_2008.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/annual/2007letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/annual/2006.PDF',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/annual/shareholderletter2005.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/annual/2004_shareholderLetter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/annual/2003_-Shareholder_-Letter041304.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/annual/2002_shareholderLetter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/annual/2001_shareholderLetter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/annual/00ar_letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/annual/Shareholderletter99.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/annual/Shareholderletter98.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/annual/Shareholderletter97.pdf',\n",
    "]\n",
    "\n",
    "# Corresponding output filenames\n",
    "filenames = [\n",
    "    'AMZN-2024-Shareholder-Letter.pdf',\n",
    "    'AMZN-2023-Shareholder-Letter.pdf',\n",
    "    'AMZN-2022-Shareholder-Letter.pdf',\n",
    "    'AMZN-2021-Shareholder-Letter.pdf',\n",
    "    'AMZN-2020-Shareholder-Letter-and-1997-Reprint.pdf',\n",
    "    'AMZN-2019-Shareholder-Letter.pdf',\n",
    "    'AMZN-2018-Shareholder-Letter.pdf',\n",
    "    'AMZN-2017-Shareholder-Letter.pdf',\n",
    "    'AMZN-2016-Shareholder-Letter.pdf',\n",
    "    'AMZN-2015-Shareholder-Letter.pdf',\n",
    "    'AMZN-2014-Shareholder-Letter.pdf',\n",
    "    'AMZN-2013-Shareholder-Letter.pdf',\n",
    "    'AMZN-2012-Shareholder-Letter.pdf',\n",
    "    'AMZN-2011-Shareholder-Letter.pdf',\n",
    "    'AMZN-2010-Shareholder-Letter.pdf',\n",
    "    'AMZN-2009-Shareholder-Letter.pdf',\n",
    "    'AMZN-2008-Shareholder-Letter.pdf',\n",
    "    'AMZN-2007-Shareholder-Letter.pdf',\n",
    "    'AMZN-2006-Shareholder-Letter.pdf',\n",
    "    'AMZN-2005-Shareholder-Letter.pdf',\n",
    "    'AMZN-2004-Shareholder-Letter.pdf',\n",
    "    'AMZN-2003-Shareholder-Letter.pdf',\n",
    "    'AMZN-2002-Shareholder-Letter.pdf',\n",
    "    'AMZN-2001-Shareholder-Letter.pdf',\n",
    "    'AMZN-2000-Shareholder-Letter.pdf',\n",
    "    'AMZN-1999-Shareholder-Letter.pdf',\n",
    "    'AMZN-1998-Shareholder-Letter.pdf',\n",
    "    'AMZN-1997-Shareholder-Letter.pdf',\n",
    "]\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(local_data_dir, exist_ok=True)\n",
    "\n",
    "# Download each file\n",
    "for url, filename in zip(urls, filenames):\n",
    "    file_path = os.path.join(local_data_dir, filename)\n",
    "    urlretrieve(url, file_path)\n",
    "    print(f\"Downloaded: '{filename}' to '{local_data_dir}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, _, files in os.walk(local_data_dir):\n",
    "    for file in files:\n",
    "        full_path = os.path.join(root, file)\n",
    "        s3_client.upload_file(full_path, s3_bucket_name, file)\n",
    "        print(f\"Uploaded: '{file}' to 's3://{s3_bucket_name}'..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Setup AOSS Vector Index and Configure BKB Access Permissions\n",
    "\n",
    "In this section, we\u2019ll create a vector index using Amazon OpenSearch Serverless (AOSS) and configure the necessary access permissions for the Bedrock Knowledge Base (BKB) that we\u2019ll set up later. AOSS provides a fully managed, serverless solution for running vector search workloads at billion-vector scale. It automatically handles resource scaling and eliminates the need for cluster management, while delivering low-latency, millisecond response times with pay-per-use pricing.\n",
    "\n",
    "While this example uses AOSS, it\u2019s worth noting that Bedrock Knowledge Bases also supports other popular vector stores, including Amazon Aurora PostgreSQL with pgvector, Pinecone, Redis Enterprise Cloud, and MongoDB, among others\n",
    "\n",
    "### 3.1 Create IAM Role with Necessary Permissions for Bedrock Knowledge Base\n",
    "\n",
    "Let's first create an IAM role with all the necessary policies and permissions to allow BKB to execute operations, such as invoking Bedrock FMs and reading data from an S3 bucket. We will use a helper function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bedrock_kb_execution_role = utility.create_bedrock_execution_role(bucket_name=s3_bucket_name)\n",
    "bedrock_kb_execution_role_arn = bedrock_kb_execution_role['Role']['Arn']\n",
    "\n",
    "print(\"Created KB execution role with ARN:\", bedrock_kb_execution_role_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Create AOSS Policies and Vector Collection\n",
    "\n",
    "Next we need to create and attach three key policies for securing and managing access to the AOSS collection: an encryption policy, a network access policy, and a data access policy. These policies ensure proper encryption, network security, and the necessary permissions for creating, reading, updating, and deleting collection items and indexes. This step is essential for configuring the OpenSearch collection to interact with BKB securely and efficiently (you can read more about AOSS collections [here](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless.html)). We will use another helper function for this.\n",
    "\n",
    "\u26a0\ufe0f **Note:** _in order to keep setup overhead at mininum, in this example we **allow public internet access** to the OpenSearch Serverless collection resource. However, for production environments we strongly suggest to leverage private connection between your VPC and Amazon OpenSearch Serverless resources via an VPC endpoint, as described [here](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless-vpc.html)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create AOSS policies for the new vector collection\n",
    "aoss_encryption_policy, aoss_network_policy, aoss_access_policy = utility.create_policies_in_oss(\n",
    "    vector_store_name=aoss_collection_name,\n",
    "    aoss_client=aoss_client,\n",
    "    bedrock_kb_execution_role_arn=bedrock_kb_execution_role_arn)\n",
    "\n",
    "print(\"Created encryption policy with name:\", aoss_encryption_policy['securityPolicyDetail']['name'])\n",
    "print(\"Created network policy with name:\", aoss_network_policy['securityPolicyDetail']['name'])\n",
    "print(\"Created access policy with name:\", aoss_access_policy['accessPolicyDetail']['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the necessary policies in place, let's proceed to actually creating a new AOSS collection. Please note that this can take a **few minutes to complete**. While you wait, you may want to [explore the AOSS Console](https://console.aws.amazon.com/aos/home?#opensearch/collections), where you will see your AOSS collection being created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request to create AOSS collection\n",
    "aoss_collection = aoss_client.create_collection(name=aoss_collection_name, type='VECTORSEARCH')\n",
    "\n",
    "# Wait until collection becomes active\n",
    "print(\"Waiting until AOSS collection becomes active: \", end='')\n",
    "while True:\n",
    "    response = aoss_client.list_collections(collectionFilters={'name': aoss_collection_name})\n",
    "    status = response['collectionSummaries'][0]['status']\n",
    "    if status in ('ACTIVE', 'FAILED'):\n",
    "        print(\" done.\")\n",
    "        break\n",
    "    print('\u2588', end='', flush=True)\n",
    "    time.sleep(5)\n",
    "\n",
    "print(\"An AOSS collection created:\", json.dumps(response['collectionSummaries'], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Grant BKB Access to AOSS Data\n",
    "\n",
    "In this step, we create a data access policy that grants BKB the necessary permissions to read from our AOSS collections. We then attach this policy to the Bedrock execution role we created earlier, allowing BKB to securely access AOSS data when generating responses. We will be using helper function once again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aoss_policy_arn = utility.create_oss_policy_attach_bedrock_execution_role(\n",
    "    collection_id=aoss_collection['createCollectionDetail']['id'],\n",
    "    bedrock_kb_execution_role=bedrock_kb_execution_role)\n",
    "\n",
    "print(\"Waiting 60 sec for data access rules to be enforced: \", end='')\n",
    "for _ in range(12):  # 12 * 5 sec = 60 sec\n",
    "    print('\u2588', end='', flush=True)\n",
    "    time.sleep(5)\n",
    "print(\" done.\")\n",
    "\n",
    "print(\"Created and attached policy with ARN:\", aoss_policy_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Create an AOSS Vector Index\n",
    "\n",
    "Now that we have all necessary access permissions in place, we can create a vector index in the AOSS collection we created previously.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests_aws4auth import AWS4Auth\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "\n",
    "# Use default credential configuration for authentication\n",
    "credentials = boto_session.get_credentials()\n",
    "awsauth = AWS4Auth(\n",
    "    credentials.access_key,\n",
    "    credentials.secret_key,\n",
    "    aws_region,\n",
    "    'aoss',\n",
    "    session_token=credentials.token)\n",
    "\n",
    "# Construct AOSS endpoint host\n",
    "host = f\"{aoss_collection['createCollectionDetail']['id']}.{aws_region}.aoss.amazonaws.com\"\n",
    "\n",
    "# Build the OpenSearch client\n",
    "os_client = OpenSearch(\n",
    "    hosts=[{'host': host, 'port': 443}],\n",
    "    http_auth=awsauth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection,\n",
    "    timeout=300\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to first define the index definiton with the desired indexing configuration, where we specify such things as number of shards and replicas of the index, vector embedding dimensions, the vector search engine (we are using FAISS here), as well as names and types of any other fields we need to have in the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the configuration for the AOSS vector index\n",
    "index_definition = {\n",
    "   \"settings\": {\n",
    "      \"index.knn\": \"true\",\n",
    "       \"number_of_shards\": 1,\n",
    "       \"knn.algo_param.ef_search\": 512,\n",
    "       \"number_of_replicas\": 0,\n",
    "   },\n",
    "   \"mappings\": {\n",
    "      \"properties\": {\n",
    "         \"vector\": {\n",
    "            \"type\": \"knn_vector\",\n",
    "            \"dimension\": embedding_model_dim,\n",
    "             \"method\": {\n",
    "                 \"name\": \"hnsw\",\n",
    "                 \"engine\": \"faiss\",\n",
    "                 \"space_type\": \"l2\"\n",
    "             },\n",
    "         },\n",
    "         \"text\": {\n",
    "            \"type\": \"text\"\n",
    "         },\n",
    "         \"text-metadata\": {\n",
    "            \"type\": \"text\"\n",
    "         }\n",
    "      }\n",
    "   }\n",
    "}\n",
    "\n",
    "# Create an OpenSearch index\n",
    "response = os_client.indices.create(index=aoss_index_name, body=index_definition)\n",
    "\n",
    "# Waiting for index creation to propagate\n",
    "print(\"Waiting 30 sec for index update to propagate: \", end='')\n",
    "for _ in range(6):  # 6 * 5 sec = 30 sec\n",
    "    print('\u2588', end='', flush=True)\n",
    "    time.sleep(5)\n",
    "print(\" done.\")\n",
    "\n",
    "print(\"A new AOSS index created:\", json.dumps(response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configure Amazon Bedrock Knowledge Base and Synchronize it with Data Source\n",
    "\n",
    "In this section, we\u2019ll create an Amazon Bedrock Knowledge Base (BKB) and connect it to the data that will be stored in our newly created AOSS vector index.\n",
    "\n",
    "### 4.1 Create a Bedrock Knowledge Base\n",
    "\n",
    "Setting up a Knowledge Base involves providing two key configurations:\n",
    "1. **Storage Configuration** tells Bedrock where to store the generated vector embeddings by specifying the target vector store and providing the necessary connection detail (here, we use the AOSS vector index we created earlier),\n",
    "2. **Knowledge Base Configuration** defines how Bedrock should generate vector embeddings from your data by specifying the embedding model to use (`Titan Text Embeddings V2` in this sample), along with any additional settings required for handling multimodal content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Knowledge Base Configuration\n",
    "knowledge_base_config = {\n",
    "    \"type\": \"VECTOR\",\n",
    "    \"vectorKnowledgeBaseConfiguration\": {\n",
    "        \"embeddingModelArn\": embedding_model_arn\n",
    "    }\n",
    "}\n",
    "\n",
    "# Storage Configuration  \n",
    "storage_config = {\n",
    "    \"type\": \"OPENSEARCH_SERVERLESS\",\n",
    "    \"opensearchServerlessConfiguration\": {\n",
    "        \"collectionArn\": aoss_collection['createCollectionDetail']['arn'],\n",
    "        \"vectorIndexName\": aoss_index_name,\n",
    "        \"fieldMapping\": {\n",
    "            \"vectorField\": \"vector\",\n",
    "            \"textField\": \"text\",\n",
    "            \"metadataField\": \"text-metadata\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Check if Knowledge Base already exists\n",
    "try:\n",
    "    # List existing knowledge bases to see if ours already exists\n",
    "    existing_kbs = bedrock_agent_client.list_knowledge_bases()\n",
    "    existing_kb = None\n",
    "    \n",
    "    for kb in existing_kbs['knowledgeBaseSummaries']:\n",
    "        if kb['name'] == bedrock_kb_name:\n",
    "            existing_kb = kb\n",
    "            break\n",
    "    \n",
    "    if existing_kb:\n",
    "        print(f\"Knowledge Base '{bedrock_kb_name}' already exists with ID: {existing_kb['knowledgeBaseId']}\")\n",
    "        print(\"Using existing Knowledge Base...\")\n",
    "        bedrock_kb_id = existing_kb['knowledgeBaseId']\n",
    "        \n",
    "        # Check if it's active\n",
    "        response = bedrock_agent_client.get_knowledge_base(knowledgeBaseId=bedrock_kb_id)\n",
    "        if response['knowledgeBase']['status'] == 'ACTIVE':\n",
    "            print(\"Existing Knowledge Base is already active.\")\n",
    "        else:\n",
    "            print(\"Waiting for existing Knowledge Base to become active: \", end='')\n",
    "            while True:\n",
    "                response = bedrock_agent_client.get_knowledge_base(knowledgeBaseId=bedrock_kb_id)\n",
    "                if response['knowledgeBase']['status'] == 'ACTIVE':\n",
    "                    print(\" done.\")\n",
    "                    break\n",
    "                print('\u2588', end='', flush=True)\n",
    "                time.sleep(5)\n",
    "    else:\n",
    "        # Create new Knowledge Base\n",
    "        response = bedrock_agent_client.create_knowledge_base(\n",
    "            name=bedrock_kb_name,\n",
    "            description=\"Amazon shareholder letter knowledge base.\",\n",
    "            roleArn=bedrock_kb_execution_role_arn,\n",
    "            knowledgeBaseConfiguration=knowledge_base_config,\n",
    "            storageConfiguration=storage_config)\n",
    "\n",
    "        bedrock_kb_id = response['knowledgeBase']['knowledgeBaseId']\n",
    "\n",
    "        print(\"Waiting until BKB becomes active: \", end='')\n",
    "        while True:\n",
    "            response = bedrock_agent_client.get_knowledge_base(knowledgeBaseId=bedrock_kb_id)\n",
    "            if response['knowledgeBase']['status'] == 'ACTIVE':\n",
    "                print(\" done.\")\n",
    "                break\n",
    "            print('\u2588', end='', flush=True)\n",
    "            time.sleep(5)\n",
    "\n",
    "        print(\"A new Bedrock Knowledge Base created with ID:\", bedrock_kb_id)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call a Bedrock API to get the information about our newly created Knowledge Base:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock_agent_client.get_knowledge_base(knowledgeBaseId=bedrock_kb_id)\n",
    "\n",
    "print(json.dumps(response['knowledgeBase'], indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.5 Understanding Chunking Strategies\n",
    "\n",
    "Before we connect our Knowledge Base to a data source, it's crucial to understand **chunking strategies** \u2014 one of the most important configuration decisions in RAG systems.\n",
    "\n",
    "**What is Chunking?**\n",
    "\n",
    "When documents are ingested into a Knowledge Base, they cannot be stored as single, monolithic texts. Instead, they must be split into smaller \"chunks\" that can be:\n",
    "- Converted into vector embeddings\n",
    "- Retrieved independently based on semantic similarity\n",
    "- Injected into LLM prompts without exceeding context limits\n",
    "\n",
    "The chunking strategy determines **how** this splitting occurs, which directly impacts:\n",
    "- **Retrieval accuracy**: Whether the right information is found\n",
    "- **Response quality**: Whether the LLM has sufficient context\n",
    "- **Cost**: Number of chunks = number of embeddings stored\n",
    "- **Latency**: Smaller chunks = faster retrieval but potentially less context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Available Chunking Strategies in Amazon Bedrock Knowledge Bases\n",
    "\n",
    "Amazon Bedrock supports four chunking strategies:\n",
    "\n",
    "| Strategy | Description | Best For | Trade-offs |\n",
    "|----------|-------------|----------|------------|\n",
    "| **FIXED_SIZE** | Splits text into equal-sized chunks based on token count with configurable overlap | General-purpose documents, consistent structure | May split semantic units (sentences/paragraphs) |\n",
    "| **NONE** | No chunking; each document becomes one chunk | Short documents (<512 tokens), pre-chunked data | Not suitable for long documents; may exceed embedding limits |\n",
    "| **HIERARCHICAL** | Creates parent-child chunk relationships; retrieves child, returns parent context | Documents with clear structure (sections, chapters) | More complex, requires structured content |\n",
    "| **SEMANTIC** | Uses AI to identify natural semantic boundaries; chunks based on topic shifts | Unstructured content, narratives, varied topics | Higher ingestion cost, slower processing |\n",
    "\n",
    "**Current Configuration**: In this workshop, we use **FIXED_SIZE** with:\n",
    "- `maxTokens: 512` - Each chunk contains up to 512 tokens\n",
    "- `overlapPercentage: 20` - 20% overlap between adjacent chunks (prevents context loss at boundaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why Use Overlap?\n",
    "\n",
    "Consider this example from an Amazon shareholder letter:\n",
    "\n",
    "**Without Overlap (0%)**:\n",
    "- Chunk 1: \"...we launched Amazon Prime in 2005. This program transformed customer expectations\"\n",
    "- Chunk 2: \"and created unprecedented loyalty. Members shop more frequently...\"\n",
    "\n",
    "**With 20% Overlap**:\n",
    "- Chunk 1: \"...we launched Amazon Prime in 2005. This program transformed customer expectations and created unprecedented loyalty.\"\n",
    "- Chunk 2: \"This program transformed customer expectations and created unprecedented loyalty. Members shop more frequently...\"\n",
    "\n",
    "**Query**: \"How did Amazon Prime affect customer loyalty?\"\n",
    "\n",
    "With overlap, Chunk 2 includes the critical context that \"this program\" refers to Amazon Prime, improving retrieval accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practical Comparison: Shareholder Letters\n",
    "\n",
    "Let's analyze how different strategies would handle our Amazon shareholder letters:\n",
    "\n",
    "**Example Text** (from 2024 letter):\n",
    "> \"Our AWS business continued its strong momentum in Q4, with revenue reaching $26.3 billion, up 13% year-over-year. The AI and machine learning services within AWS saw particularly strong adoption, with customers leveraging Amazon Bedrock and SageMaker for their generative AI initiatives.\"\n",
    "\n",
    "**FIXED_SIZE (512 tokens, 20% overlap)**:\n",
    "- \u2705 Predictable chunk sizes\n",
    "- \u2705 Works well for financial data with numbers\n",
    "- \u26a0\ufe0f Might split \"AWS revenue\" from \"year-over-year comparison\"\n",
    "\n",
    "**SEMANTIC**:\n",
    "- \u2705 Keeps \"AWS revenue\" and \"year-over-year growth\" together\n",
    "- \u2705 Groups related services (Bedrock, SageMaker) in one chunk\n",
    "- \u26a0\ufe0f Variable chunk sizes might affect consistency\n",
    "\n",
    "**HIERARCHICAL**:\n",
    "- \u2705 Could treat each year's letter as parent, each section as child\n",
    "- \u2705 Retrieves specific metric but returns full section context\n",
    "- \u26a0\ufe0f Requires structured PDFs with clear section markers\n",
    "\n",
    "**NONE**:\n",
    "- \u274c Each full shareholder letter (5-10 pages) becomes one chunk\n",
    "- \u274c Exceeds typical embedding model limits (512-1024 tokens)\n",
    "- \u274c Not recommended for this use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEMONSTRATION: How different chunking strategies affect the same text\n",
    "# Note: This is a simulation for educational purposes\n",
    "\n",
    "sample_text = \"\"\"\n",
    "Amazon Web Services (AWS) continued to show strong performance throughout 2024. \n",
    "The cloud infrastructure business grew revenue by 13% year-over-year, reaching $26.3 billion in Q4 alone.\n",
    "\n",
    "Our AI and machine learning services saw exceptional adoption. Amazon Bedrock, our fully managed \n",
    "generative AI service, enabled thousands of customers to build innovative applications. SageMaker \n",
    "customers increased by 45% as organizations accelerated their ML initiatives.\n",
    "\n",
    "Looking forward, we remain focused on three key areas: expanding our infrastructure footprint, \n",
    "enhancing our AI/ML capabilities, and deepening customer relationships through innovation.\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "\n",
    "def simulate_fixed_size_chunking(text, max_tokens=50, overlap_pct=20):\n",
    "    \"\"\"Simulates FIXED_SIZE chunking strategy\"\"\"\n",
    "    # Simplified: using words as proxy for tokens (1 word \u2248 1.3 tokens)\n",
    "    words = text.split()\n",
    "    max_words = int(max_tokens / 1.3)\n",
    "    overlap_words = int(max_words * overlap_pct / 100)\n",
    "    \n",
    "    chunks = []\n",
    "    i = 0\n",
    "    chunk_num = 1\n",
    "    \n",
    "    while i < len(words):\n",
    "        chunk_end = min(i + max_words, len(words))\n",
    "        chunk = ' '.join(words[i:chunk_end])\n",
    "        chunks.append(f\"Chunk {chunk_num}: {chunk}...\")\n",
    "        \n",
    "        # Move forward by (chunk_size - overlap)\n",
    "        i += (max_words - overlap_words)\n",
    "        chunk_num += 1\n",
    "        \n",
    "        if i >= len(words):\n",
    "            break\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def simulate_semantic_chunking(text):\n",
    "    \"\"\"Simulates SEMANTIC chunking strategy\"\"\"\n",
    "    # Simplified: splits on paragraph boundaries (semantic breaks)\n",
    "    paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
    "    chunks = [f\"Semantic Chunk {i+1}: {p}\" for i, p in enumerate(paragraphs)]\n",
    "    return chunks\n",
    "\n",
    "def simulate_no_chunking(text):\n",
    "    \"\"\"Simulates NONE strategy\"\"\"\n",
    "    return [f\"Single Chunk (NONE strategy): {text}\"]\n",
    "\n",
    "# Compare strategies\n",
    "print(\"=\"*80)\n",
    "print(\"FIXED_SIZE Chunking (50 tokens, 20% overlap)\")\n",
    "print(\"=\"*80)\n",
    "for chunk in simulate_fixed_size_chunking(sample_text):\n",
    "    print(f\"\\n{chunk}\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SEMANTIC Chunking (natural boundaries)\")\n",
    "print(\"=\"*80)\n",
    "for chunk in simulate_semantic_chunking(sample_text):\n",
    "    print(f\"\\n{chunk}\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"NONE Strategy (no chunking)\")\n",
    "print(\"=\"*80)\n",
    "for chunk in simulate_no_chunking(sample_text):\n",
    "    print(f\"\\n{chunk}\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Analysis:\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "FIXED_SIZE:\n",
    "  - Created multiple overlapping chunks\n",
    "  - Consistent size enables predictable retrieval\n",
    "  - Overlap preserves context at boundaries\n",
    "\n",
    "SEMANTIC:\n",
    "  - Aligned with natural topic breaks\n",
    "  - Variable sizes (AWS performance, AI/ML services, Future focus)\n",
    "  - Each chunk is semantically complete\n",
    "\n",
    "NONE:\n",
    "  - Entire text as single chunk\n",
    "  - Only suitable if text is already small (<512 tokens)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chunking Best Practices\n",
    "\n",
    "**For Financial Documents (like shareholder letters)**:\n",
    "- \u2705 **Recommended**: FIXED_SIZE with 512 tokens, 20% overlap\n",
    "- **Rationale**: Financial data benefits from consistent chunk sizes; overlap ensures metrics stay with their context\n",
    "- **Alternative**: SEMANTIC for narrative sections, but FIXED_SIZE for tables/metrics\n",
    "\n",
    "**General Guidelines**:\n",
    "\n",
    "1. **Chunk Size Selection**:\n",
    "   - **300-512 tokens**: Optimal for most use cases (balances context vs. precision)\n",
    "   - **Less than 300**: Risk of insufficient context\n",
    "   - **More than 800**: Risk of retrieving irrelevant information\n",
    "\n",
    "2. **Overlap Percentage**:\n",
    "   - **15-20%**: Standard overlap for general content\n",
    "   - **30-40%**: Higher overlap for dense technical content\n",
    "   - **0-10%**: Lower overlap for clearly structured documents\n",
    "\n",
    "3. **Strategy Selection Decision Tree**:\n",
    "   ```\n",
    "   Is your content pre-chunked? (e.g., Q&A pairs)\n",
    "       YES \u2192 Use NONE\n",
    "       NO \u2192 Continue\n",
    "   \n",
    "   Does your content have clear hierarchical structure? (e.g., documentation with sections)\n",
    "       YES \u2192 Consider HIERARCHICAL\n",
    "       NO \u2192 Continue\n",
    "   \n",
    "   Is your content highly unstructured with varying topics? (e.g., emails, transcripts)\n",
    "       YES \u2192 Consider SEMANTIC\n",
    "       NO \u2192 Use FIXED_SIZE (safest default)\n",
    "   ```\n",
    "\n",
    "4. **Cost Considerations**:\n",
    "   - Smaller chunks = More embeddings = Higher storage cost\n",
    "   - Balance chunk size with retrieval quality needs\n",
    "   - For our 28 shareholder letters: ~500-700 chunks expected with FIXED_SIZE\n",
    "\n",
    "#### What Happens Next?\n",
    "\n",
    "When we create our data source in the next cell, Bedrock will:\n",
    "1. Read each PDF from S3\n",
    "2. Extract text content\n",
    "3. Apply the FIXED_SIZE chunking strategy (512 tokens, 20% overlap)\n",
    "4. Generate embeddings for each chunk using Titan Embeddings V2\n",
    "5. Store embeddings in our AOSS vector index\n",
    "\n",
    "Let's proceed to configure the data source!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \ud83c\udfaf Learning Checkpoint: Chunking Strategies\n",
    "\n",
    "Before proceeding, test your understanding:\n",
    "\n",
    "**Question 1**: If a document is 2,000 tokens long and we use FIXED_SIZE chunking with maxTokens=512 and overlapPercentage=20, approximately how many chunks will be created?\n",
    "\n",
    "<details>\n",
    "<summary>Click to see answer</summary>\n",
    "\n",
    "**Answer**: Approximately 4-5 chunks\n",
    "\n",
    "**Explanation**:\n",
    "- Each chunk is 512 tokens\n",
    "- Overlap is 20% = ~102 tokens\n",
    "- Effective advancement per chunk = 512 - 102 = 410 tokens\n",
    "- 2000 / 410 \u2248 4.9 chunks\n",
    "\n",
    "Calculation:\n",
    "- Chunk 1: tokens 0-512\n",
    "- Chunk 2: tokens 410-922 (102 overlap from chunk 1)\n",
    "- Chunk 3: tokens 820-1332\n",
    "- Chunk 4: tokens 1230-1742\n",
    "- Chunk 5: tokens 1640-2000 (partial)\n",
    "\n",
    "</details>\n",
    "\n",
    "**Question 2**: Why might SEMANTIC chunking be slower than FIXED_SIZE during ingestion?\n",
    "\n",
    "<details>\n",
    "<summary>Click to see answer</summary>\n",
    "\n",
    "**Answer**: SEMANTIC chunking uses AI models to identify topic boundaries, requiring additional inference calls.\n",
    "\n",
    "**Explanation**:\n",
    "- FIXED_SIZE: Simple token counting (fast)\n",
    "- SEMANTIC: Analyzes text semantics using ML models (slower but more intelligent)\n",
    "- Trade-off: Better semantic coherence vs. longer ingestion time\n",
    "\n",
    "</details>\n",
    "\n",
    "**Question 3**: When would you choose NONE as your chunking strategy?\n",
    "\n",
    "<details>\n",
    "<summary>Click to see answer</summary>\n",
    "\n",
    "**Answer**: When documents are already small (<512 tokens) or pre-chunked into semantic units.\n",
    "\n",
    "**Examples**:\n",
    "- FAQ pairs where each Q&A is already a complete unit\n",
    "- Product descriptions (50-200 tokens each)\n",
    "- Pre-processed data where you've already done custom chunking\n",
    "- Email subjects and bodies stored separately\n",
    "\n",
    "**Warning**: NONE with large documents will fail if they exceed embedding model limits.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Connect BKB to a Data Source\n",
    "\n",
    "With our Knowledge Base in place, the next step is to connect it to a data source. This involves two key actions:\n",
    "\n",
    "1. **Create a data source for the Knowledge Base** that will point to the location of our raw data (in this case, S3),\n",
    "2. **Define how that data should be processed and ingested into the vector store** \u2014 for example, by specifying a chunking configuration that controls how large each text fragment should be when generating vector embeddings for retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Source Configuration\n",
    "data_source_config = {\n",
    "        \"type\": \"S3\",\n",
    "        \"s3Configuration\":{\n",
    "            \"bucketArn\": f\"arn:aws:s3:::{s3_bucket_name}\",\n",
    "            # \"inclusionPrefixes\":[\"*.*\"]   # you can use this if you want to create a KB using data within s3 prefixes.\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Vector Ingestion Configuration\n",
    "vector_ingestion_config = {\n",
    "        \"chunkingConfiguration\": {\n",
    "            \"chunkingStrategy\": \"FIXED_SIZE\",\n",
    "            \"fixedSizeChunkingConfiguration\": {\n",
    "                \"maxTokens\": 512,\n",
    "                \"overlapPercentage\": 20\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "response = bedrock_agent_client.create_data_source(\n",
    "    name=bedrock_kb_name,\n",
    "    description=\"Amazon shareholder letter knowledge base.\",\n",
    "    knowledgeBaseId=bedrock_kb_id,\n",
    "    dataSourceConfiguration=data_source_config,\n",
    "    vectorIngestionConfiguration=vector_ingestion_config\n",
    ")\n",
    "\n",
    "bedrock_ds_id = response['dataSource']['dataSourceId']\n",
    "\n",
    "print(\"A new BKB data source created with ID:\", bedrock_ds_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also use Bedrock API to get the information about our newly created BKB data source:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = bedrock_agent_client.get_data_source(knowledgeBaseId=bedrock_kb_id, dataSourceId=bedrock_ds_id)\n",
    "\n",
    "print(json.dumps(response['dataSource'], indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Synchronize BKB with Data Source\n",
    "\n",
    "Once the Knowledge Base and its data source are configured, we can start a fully-managed data ingestion job. During this process, BKB will retrieve the documents from the connected data source (on S3, in this case), extract and preprocess the content, split it into smaller chunks based on the configured chunking strategy, generate vector embeddings for each chunk, and store those embeddings in the vector store (AOSS vector store, in this case).\n",
    "\n",
    "![BKB data ingestion](./images/data_ingestion.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start an ingestion job\n",
    "response = bedrock_agent_client.start_ingestion_job(knowledgeBaseId=bedrock_kb_id, dataSourceId=bedrock_ds_id)\n",
    "\n",
    "bedrock_job_id = response['ingestionJob']['ingestionJobId']\n",
    "\n",
    "print(\"A new BKB ingestion job started with ID:\", bedrock_job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Wait until ingestion job completes\n",
    "print(\"Waiting until BKB ingestion job completes: \", end='')\n",
    "while True:\n",
    "    response = bedrock_agent_client.get_ingestion_job(\n",
    "        knowledgeBaseId = bedrock_kb_id,\n",
    "        dataSourceId = bedrock_ds_id,\n",
    "        ingestionJobId = bedrock_job_id)\n",
    "    if response['ingestionJob']['status'] == 'COMPLETE':\n",
    "        print(\" done.\")\n",
    "        break\n",
    "    print('\u2588', end='', flush=True)\n",
    "    time.sleep(5)\n",
    "\n",
    "print(\"The BKB ingestion job finished:\", json.dumps(response['ingestionJob'], indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusions and Next Steps\n",
    "\n",
    "In this notebook, we walked through the process of creating an Amazon Bedrock Knowledge Base (BKB) and ingesting documents to enable Retrieval Augmented Generation (RAG) capabilities. We started by setting up the environment, installing the required libraries, and initializing the necessary AWS service clients. Then, we created an Amazon S3 bucket to store unstructured data (PDF documents) and uploaded sample files. We proceeded by provisioning an Amazon OpenSearch Serverless (AOSS) collection and index, configuring the appropriate IAM roles and permissions, and granting access to the BKB. Finally, we created the BKB, connected it to the S3 data source, and synchronized the documents to generate vector embeddings, which were stored in AOSS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "Please execute next cell to store some important varables that will be needed in other notebooks of this module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store s3_bucket_name aoss_encryption_policy aoss_network_policy aoss_access_policy aoss_collection bedrock_kb_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now please go on to explore how you can interact with the newly created Knowledge Base via Bedrock APIs for RAG applications, please proceed to the next notebook:\n",
    "\n",
    "&nbsp; **NEXT \u25b6** [2_managed-rag-with-retrieve-and-generate-api.ipynb](./2\\_managed-rag-with-retrieve-and-generate-api.ipynb)."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}