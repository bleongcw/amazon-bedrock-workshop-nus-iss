{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2 - Query Knowledge Base and Build RAG-powered Q&A Application with **Retrieve API**\n",
    "\n",
    "----\n",
    "\n",
    "This notebook provides sample code and step-by-step instructions for building a question-answering (Q&A) application using a **Retrieve API** of Amazon Bedrock Knowledge Bases.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "In the previous notebook, we explored the `RetrieveAndGenerate` API from Amazon Bedrock Knowledge Bases \u2014 a fully managed RAG (Retrieval-Augmented Generation) solution. As the name suggests, this API not only retrieves the most relevant information from a knowledge base but also automatically generates a response to the user query in a single, fully managed API call.\n",
    "\n",
    "In this notebook, we will take a closer look at the `Retrieve` API, which provides greater flexibility for building custom RAG solutions. Unlike `RetrieveAndGenerate`, the `Retrieve` API only fetches relevant document chunks from a Knowledge Base based on the user query \u2014 leaving it up to the developer to decide how to leverage this retrieved information.\n",
    "\n",
    "To keep things simple and focused, in this notebook we will use the output of the `Retrieve` API to manually construct an augmented prompt. We will then send this prompt to a Bedrock's foundation model (FM) of our choice to generate a grounded response.\n",
    "\n",
    "![retrieveAPI](./images/retrieve_api.png)\n",
    "\n",
    "### Pre-requisites\n",
    "\n",
    "In order to run this notebook, you should have successfully completed the first notebook lab:\n",
    "- [1_create-kb-and-ingest-documents.ipynb](./1\\_create-kb-and-ingest-documents.ipynb).\n",
    "\n",
    "Also, please make sure that you have enabled the following model access in _Amazon Bedrock Console_:\n",
    "\n",
    "- `Amazon Nova Micro`\n",
    "- `Amazon Titan Text Embeddings V2`\n",
    "\n",
    "## 1. Setup\n",
    "\n",
    "### 1.1 Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Third-party imports\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Local imports\n",
    "import utility\n",
    "\n",
    "# Print SDK versions\n",
    "print(f\"Python version: {sys.version.split()[0]}\")\n",
    "print(f\"Boto3 SDK version: {boto3.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Initial setup for clients and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r bedrock_kb_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boto3 session and set AWS region\n",
    "boto_session = boto3.Session()\n",
    "aws_region = boto_session.region_name\n",
    "\n",
    "# Create boto3 clients for Bedrock\n",
    "bedrock_config = Config(connect_timeout=120, read_timeout=120, retries={'max_attempts': 0})\n",
    "bedrock_client = boto3.client('bedrock-runtime')\n",
    "bedrock_agent_client = boto3.client('bedrock-agent-runtime', config=bedrock_config)\n",
    "\n",
    "# Set the Bedrock model to use for text generation\n",
    "model_id = 'amazon.nova-micro-v1:0'\n",
    "model_arn = f'arn:aws:bedrock:{aws_region}::foundation-model/{model_id}'\n",
    "\n",
    "# Print configurations\n",
    "print(\"AWS Region:\", aws_region)\n",
    "print(\"Bedrock Knowledge Base ID:\", bedrock_kb_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using the **Retrieve API** with Foundation Models from Amazon Bedrock\n",
    "\n",
    "We will begin by defining a `retrieve` function that calls the `Retrieve` API provided by Amazon Bedrock Knowledge Bases (BKB). This API transforms the user query into vector embeddings, searches the connected knowledge base, and returns the most relevant results. This approach gives you fine-grained control to build custom RAG workflows on top of the retrieved content.\n",
    "\n",
    "The response from the `Retrieve` API includes several useful components:\n",
    "\n",
    "- The **retrieved document chunks** containing relevant content from the knowledge base  \n",
    "- The **source location type** and **URI** for each retrieved document, enabling traceability  \n",
    "- The **relevance score** for each document chunk, indicating how well it matches the query  \n",
    "\n",
    "Additionally, the `Retrieve` API supports the `overrideSearchType` parameter within `retrievalConfiguration`, allowing you to control the search strategy used:\n",
    "\n",
    "| Search Type | Description |\n",
    "|-------------|-------------|\n",
    "| `HYBRID`    | Combines semantic search (vector similarity) with keyword search for improved accuracy, especially for structured content. |\n",
    "| `SEMANTIC`  | Purely embedding-based semantic search, ideal for unstructured or natural language content. |\n",
    "\n",
    "By default, Amazon Bedrock automatically selects the optimal search strategy for your query. However, if needed, you can explicitly specify `HYBRID` or `SEMANTIC` using `overrideSearchType` to tailor the search behavior to your use case.\n",
    "\n",
    "### 2.1 Exploring the **Retrieve API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the `retrieve` function\n",
    "def retrieve(user_query, kb_id, num_of_results=5):\n",
    "    return bedrock_agent_client.retrieve(\n",
    "        retrievalQuery= {\n",
    "            'text': user_query\n",
    "        },\n",
    "        knowledgeBaseId=kb_id,\n",
    "        retrievalConfiguration= {\n",
    "            'vectorSearchConfiguration': {\n",
    "                'numberOfResults': num_of_results,\n",
    "                'overrideSearchType': \"SEMANTIC\", # optional\n",
    "            }\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "user_query = \"How has Amazon's businesses performed in 2024 as compared to it from 1997?\"\n",
    "\n",
    "response = retrieve(user_query, bedrock_kb_id, num_of_results=3)\n",
    "\n",
    "print(\"Retrieval Results:\\n\", json.dumps(response['retrievalResults'], indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Experiment: How numberOfResults Affects Responses\n",
    "\n",
    "One critical parameter in the `Retrieve` API is `numberOfResults` \u2014 it controls how many chunks are retrieved from the Knowledge Base. Let's experiment to understand its impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: Vary numberOfResults to see impact on response quality\n",
    "\n",
    "user_query = \"How has Amazon's businesses performed in 2024 as compared to it from 1997?\"\n",
    "\n",
    "# Test different retrieval counts\n",
    "test_configs = [\n",
    "    {\"num_results\": 3, \"description\": \"Minimal context (3 chunks)\"},\n",
    "    {\"num_results\": 10, \"description\": \"Moderate context (10 chunks)\"},\n",
    "    {\"num_results\": 20, \"description\": \"Maximum context (20 chunks)\"},\n",
    "]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for config in test_configs:\n",
    "    num_results = config['num_results']\n",
    "    description = config['description']\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Testing: {description}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Retrieve with this configuration\n",
    "    retrieve_response = retrieve(user_query, bedrock_kb_id, num_of_results=num_results)\n",
    "    \n",
    "    # Extract contexts\n",
    "    contexts = [rr['content']['text'] for rr in retrieve_response['retrievalResults']]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    total_context_chars = sum(len(c) for c in contexts)\n",
    "    avg_relevance = sum(rr.get('score', 0) for rr in retrieve_response['retrievalResults']) / num_results\n",
    "    \n",
    "    # Store results\n",
    "    results[num_results] = {\n",
    "        'num_chunks': len(contexts),\n",
    "        'total_context_chars': total_context_chars,\n",
    "        'avg_relevance': avg_relevance,\n",
    "        'sources': [rr['location']['s3Location']['uri'] for rr in retrieve_response['retrievalResults']]\n",
    "    }\n",
    "    \n",
    "    print(f\"Chunks Retrieved: {num_results}\")\n",
    "    print(f\"Total Context Size: {total_context_chars:,} characters\")\n",
    "    print(f\"Average Relevance Score: {avg_relevance:.4f}\")\n",
    "    print(f\"Unique Sources: {len(set(results[num_results]['sources']))}\")\n",
    "\n",
    "# Comparative analysis\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARATIVE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n{'Config':<25} {'Chunks':<10} {'Context Size':<15} {'Avg Relevance':<15} {'Unique Docs':<12}\")\n",
    "print(\"-\" * 80)\n",
    "for num_results, data in results.items():\n",
    "    unique_docs = len(set(data['sources']))\n",
    "    print(f\"{num_results} results{'':<16} {data['num_chunks']:<10} {data['total_context_chars']:>12,}   {data['avg_relevance']:>12.4f}   {unique_docs:>10}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INSIGHTS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "Observations:\n",
    "\n",
    "1. Context Size Growth:\n",
    "   - More chunks = More context for the LLM\n",
    "   - But also more noise if low-relevance chunks are included\n",
    "   - Diminishing returns after ~10-15 chunks for most queries\n",
    "\n",
    "2. Relevance Score Trend:\n",
    "   - Average relevance decreases as numberOfResults increases\n",
    "   - First few chunks are most relevant\n",
    "   - Later chunks may introduce irrelevant information\n",
    "\n",
    "3. Source Diversity:\n",
    "   - Higher numberOfResults may pull from more unique documents\n",
    "   - Good for synthesis questions (comparing years)\n",
    "   - Not always necessary for focused factual queries\n",
    "\n",
    "4. Cost Implications:\n",
    "   - More chunks = larger prompt = higher inference cost\n",
    "   - Balance retrieval quality with cost efficiency\n",
    "\n",
    "Recommendation:\n",
    "- Simple factual queries: 3-5 chunks\n",
    "- Synthesis/comparison queries: 10-15 chunks\n",
    "- Broad exploratory queries: 15-20 chunks (max)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Experiment: SEMANTIC vs HYBRID Search Types\n",
    "\n",
    "Amazon Bedrock Knowledge Bases support two search types:\n",
    "- **SEMANTIC**: Pure vector similarity search\n",
    "- **HYBRID**: Combines vector similarity with keyword matching\n",
    "\n",
    "Let's compare them on the same query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: Compare SEMANTIC vs HYBRID search\n",
    "\n",
    "user_query = \"What was AWS revenue in 2024?\"\n",
    "\n",
    "search_types = ['SEMANTIC', 'HYBRID']\n",
    "search_results = {}\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"Query: {user_query}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for search_type in search_types:\n",
    "    print(f\"\\n--- Testing {search_type} Search ---\\n\")\n",
    "    \n",
    "    # Retrieve with specific search type\n",
    "    retrieve_response = bedrock_agent_client.retrieve(\n",
    "        retrievalQuery={'text': user_query},\n",
    "        knowledgeBaseId=bedrock_kb_id,\n",
    "        retrievalConfiguration={\n",
    "            'vectorSearchConfiguration': {\n",
    "                'numberOfResults': 5,\n",
    "                'overrideSearchType': search_type\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Analyze results\n",
    "    chunks = []\n",
    "    for i, result in enumerate(retrieve_response['retrievalResults'], 1):\n",
    "        chunk_text = result['content']['text']\n",
    "        score = result.get('score', 0)\n",
    "        location = result['location']['s3Location']['uri']\n",
    "        \n",
    "        chunks.append({\n",
    "            'text': chunk_text,\n",
    "            'score': score,\n",
    "            'source': location.split('/')[-1]  # Just filename\n",
    "        })\n",
    "        \n",
    "        print(f\"Chunk {i} (Score: {score:.4f}):\")\n",
    "        print(f\"  Source: {chunks[-1]['source']}\")\n",
    "        print(f\"  Preview: {chunk_text[:150]}...\")\n",
    "        print()\n",
    "    \n",
    "    search_results[search_type] = chunks\n",
    "\n",
    "# Compare results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON: SEMANTIC vs HYBRID\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Overlap analysis\n",
    "semantic_sources = set(c['source'] for c in search_results['SEMANTIC'])\n",
    "hybrid_sources = set(c['source'] for c in search_results['HYBRID'])\n",
    "overlap = semantic_sources & hybrid_sources\n",
    "\n",
    "print(f\"\\nSource Document Overlap:\")\n",
    "print(f\"  SEMANTIC found: {semantic_sources}\")\n",
    "print(f\"  HYBRID found: {hybrid_sources}\")\n",
    "print(f\"  Common sources: {overlap}\")\n",
    "print(f\"  Overlap percentage: {len(overlap) / 5 * 100:.0f}%\")\n",
    "\n",
    "# Relevance score comparison\n",
    "semantic_avg_score = sum(c['score'] for c in search_results['SEMANTIC']) / 5\n",
    "hybrid_avg_score = sum(c['score'] for c in search_results['HYBRID']) / 5\n",
    "\n",
    "print(f\"\\nAverage Relevance Scores:\")\n",
    "print(f\"  SEMANTIC: {semantic_avg_score:.4f}\")\n",
    "print(f\"  HYBRID: {hybrid_avg_score:.4f}\")\n",
    "print(f\"  Difference: {abs(semantic_avg_score - hybrid_avg_score):.4f}\")\n",
    "\n",
    "# Keyword presence analysis\n",
    "keyword_to_check = \"AWS\"\n",
    "semantic_keyword_count = sum(1 for c in search_results['SEMANTIC'] if keyword_to_check in c['text'])\n",
    "hybrid_keyword_count = sum(1 for c in search_results['HYBRID'] if keyword_to_check in c['text'])\n",
    "\n",
    "print(f\"\\nKeyword Presence ('{keyword_to_check}'):\")\n",
    "print(f\"  SEMANTIC: {semantic_keyword_count}/5 chunks contain keyword\")\n",
    "print(f\"  HYBRID: {hybrid_keyword_count}/5 chunks contain keyword\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"WHEN TO USE EACH SEARCH TYPE\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "SEMANTIC Search:\n",
    "\u2705 Best for: Conceptual questions, synonyms, paraphrasing\n",
    "\u2705 Example: \"What is Amazon's customer-centric philosophy?\"\n",
    "   - Matches \"customer obsession,\" \"customer focus,\" \"customer-first\"\n",
    "\u2705 Handles semantic similarity well\n",
    "\u274c May miss exact keyword matches\n",
    "\n",
    "HYBRID Search:\n",
    "\u2705 Best for: Queries with specific terms, acronyms, product names\n",
    "\u2705 Example: \"What was AWS revenue in 2024?\"\n",
    "   - Ensures chunks with \"AWS\" and \"2024\" are prioritized\n",
    "\u2705 Combines semantic understanding with keyword precision\n",
    "\u274c Slightly more complex ranking algorithm\n",
    "\n",
    "Default Behavior:\n",
    "- Bedrock automatically selects optimal search type if not specified\n",
    "- For most use cases, let Bedrock decide\n",
    "- Override only when you have specific requirements\n",
    "\n",
    "Recommendation for this workshop:\n",
    "- Use SEMANTIC for conceptual queries (leadership principles, strategy)\n",
    "- Use HYBRID for factual queries with specific terms (revenue, dates, products)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"How has Amazon's businesses performed in 2024 as compared to it from 1997?\"\n",
    "\n",
    "response = retrieve(user_query, bedrock_kb_id, num_of_results=10)\n",
    "\n",
    "# Combine all retrieved text into one continuous text\n",
    "all_text = \"\"\n",
    "for result in response['retrievalResults']:\n",
    "    text = result['content']['text']\n",
    "    all_text += text + \"\\n\\n\"\n",
    "\n",
    "print(\"Combined Retrieved Text:\")\n",
    "print(all_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Generating a Response using Retrieved Context and the **Converse API**\n",
    "\n",
    "Once we have used the `Retrieve` API to fetch the most relevant document chunks from our knowledge base, the next step is to use this retrieved context to generate a grounded and informative response to the user query.\n",
    "\n",
    "In this section, we will construct a LLM request that combines both user query and the retrieved knowledge base content. We will then use Amazon Bedrock's `Converse` API to interact with a LLM of our choice to generate the final response.\n",
    "\n",
    "Specifically:\n",
    "- We will define a *system prompt* that provides general behavioral guidelines to the model \u2014 for example, instructing it to act like a financial advisor that prioritizes fact-based, concise answers.\n",
    "- We will create a *user prompt template* that injects both the retrieved context and the user\u2019s query.\n",
    "- Finally, we will use the `Converse` API to generate the model\u2019s response, ensuring that it leverages the provided context to produce accurate and grounded answers.\n",
    "\n",
    "This pattern enables full control over how context is presented to the model, allowing you to implement custom RAG workflows tailored to your application's needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a system prompt\n",
    "system_prompt = \"\"\"You are a financial advisor AI system, and provides answers to questions\n",
    "by using fact based and statistical information when possible. \n",
    "Use the following pieces of information in <context> tags to provide a concise answer to the questions.\n",
    "Give an answer directly, without any XML tags.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\"\"\"\n",
    "\n",
    "# Define a user prompt template\n",
    "user_prompt_template = \"\"\"Here is some additional context:\n",
    "<context>\n",
    "{contexts}\n",
    "</context>\n",
    "\n",
    "Please provide an answer to this user query:\n",
    "<query>\n",
    "{user_query}\n",
    "</query>\n",
    "\n",
    "The response should be specific and use statistics or numbers when possible.\"\"\"\n",
    "\n",
    "# Extract all context from all relevant retrieved document chunks\n",
    "contexts = [rr['content']['text'] for rr in response['retrievalResults']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Converse API request\n",
    "converse_request = {\n",
    "    \"system\": [\n",
    "        {\"text\": system_prompt}\n",
    "    ],\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": user_prompt_template.format(contexts=contexts, user_query=user_query)\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"inferenceConfig\": {\n",
    "        \"temperature\": 0.1,\n",
    "        \"topP\": 0.95,\n",
    "        \"maxTokens\": 1000\n",
    "    }\n",
    "}\n",
    "\n",
    "# Call Bedrock's Converse API to generate the final answer to user query\n",
    "response = bedrock_client.converse(\n",
    "    modelId=model_id,\n",
    "    system=converse_request['system'],\n",
    "    messages=converse_request[\"messages\"],\n",
    "    inferenceConfig=converse_request[\"inferenceConfig\"]\n",
    ")\n",
    "\n",
    "print(\"Final Answer:\\n\", response[\"output\"][\"message\"][\"content\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Conclusions and Next Steps\n",
    "\n",
    "In this notebook, we built a custom RAG-powered Q&A application using Amazon Bedrock Knowledge Bases and the `Retrieve` API.\n",
    "\n",
    "We followed three main steps:\n",
    "- Used the `Retrieve` API to fetch the most relevant document chunks from a knowledge base based on a user query.\n",
    "- Constructed an augmented prompt by combining the retrieved content with the user\u2019s question.\n",
    "- Used the `Converse` API to generate a grounded, fact-based response leveraging the retrieved context.\n",
    "\n",
    "This approach provides flexibility and control over both search and response generation, enabling tailored RAG solutions for your specific use case.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Do not forget to clean up the resources here, if you do not indent to expriment with the created Bedrock Knowledge Base anymore:\n",
    "\n",
    "&nbsp; **NEXT \u25b6** [4_clean-up.ipynb](./4\\_clean-up.ipynb)"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}